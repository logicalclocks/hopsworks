{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"admin/","text":"Cluster Administration # Hopsworks has a cluster management page that allows you, the administrator, to perform management actions, monitor and control Hopsworks. To access the cluster management page you should log in into Hopsworks using your administrator account. In the top right corner, click on your name in the top right corner of the navigation bar and choose Cluster Settings from the dropdown menu.","title":"Introduction"},{"location":"admin/#cluster-administration","text":"Hopsworks has a cluster management page that allows you, the administrator, to perform management actions, monitor and control Hopsworks. To access the cluster management page you should log in into Hopsworks using your administrator account. In the top right corner, click on your name in the top right corner of the navigation bar and choose Cluster Settings from the dropdown menu.","title":"Cluster Administration"},{"location":"admin/alert/","text":"Configure Alerts # Alerts are sent from Hopsworks using Prometheus' Alert manager . In order to send alerts we first need to configure the Alert manager . To do that click on your name in the top right corner of the navigation bar and choose Cluster Settings from the dropdown menu. In the Cluster Settings' Alerts tab you can configure the alert manager to send alerts via email, slack or pagerduty. Configure alerts 1. Email Alerts # To send alerts via email you need to configure an SMTP server. Click on the Configure button on the left side of the email row and fill out the form that pops up. Configure Email Alerts Default from : the address used as sender in the alert email. SMTP smarthost : the Simple Mail Transfer Protocol (SMTP) host through which emails are sent. Default hostname (optional) : hostname to identify to the SMTP server. Authentication method : how to authenticate to the SMTP server. CRAM-MD5, LOGIN or PLAIN. Optionally cluster wide Email alert receivers can be added in Default receiver emails . These receivers will be available to all users when they create event triggered alerts . 2. Slack Alerts # Alert can also be sent via Slack message. To be able to send Slack messages you first need to configure a Slack webhook. Click on the Configure button on the left side of the slack row and past in your Slack webhook in Webhook . Configure slack Alerts Optionally cluster wide Slack alert receivers can be added in Slack channel/user . These receivers will be available to all users when they create event triggered alerts . 3. Pagerduty # Pagerduty is another way you can send alerts from Hopsworks. Click on the Configure button on the left side of the pagerduty row and fill out the form that pops up. Configure Pagerduty Alerts Fill in Pagerduty URL: the URL to send API requests to. Optionally cluster wide Pagerduty alert receivers can be added in Service key/Routing key . By first choosing the PagerDuty integration type: global event routing (routing_key) : when using PagerDuty integration type Events API v2 . service (service_key) : when using PagerDuty integration type Prometheus . Then adding the Service key/Routing key of the receiver(s). PagerDuty provides documentation on how to integrate with Prometheus' Alert manager. Advanced configuration # If you are familiar with Prometheus' Alert manager you can also configure alerts by editing the yaml/json file directly. Advanced configuration Example: Adding the yaml snippet shown below in the global section of the alert manager configuration will have the same effect as creating the SMTP configuration as shown in section 1 above. global : smtp_smarthost : smtp.gmail.com:587 smtp_from : hopsworks@gmail.com smtp_auth_username : hopsworks@gmail.com smtp_auth_password : XXXXXXXXX smtp_auth_identity : hopsworks@gmail.com ... To test the alerts by creating triggers from Jobs and Feature group validations see Alerts .","title":"Configure Alerts"},{"location":"admin/alert/#configure-alerts","text":"Alerts are sent from Hopsworks using Prometheus' Alert manager . In order to send alerts we first need to configure the Alert manager . To do that click on your name in the top right corner of the navigation bar and choose Cluster Settings from the dropdown menu. In the Cluster Settings' Alerts tab you can configure the alert manager to send alerts via email, slack or pagerduty. Configure alerts","title":"Configure Alerts"},{"location":"admin/alert/#1-email-alerts","text":"To send alerts via email you need to configure an SMTP server. Click on the Configure button on the left side of the email row and fill out the form that pops up. Configure Email Alerts Default from : the address used as sender in the alert email. SMTP smarthost : the Simple Mail Transfer Protocol (SMTP) host through which emails are sent. Default hostname (optional) : hostname to identify to the SMTP server. Authentication method : how to authenticate to the SMTP server. CRAM-MD5, LOGIN or PLAIN. Optionally cluster wide Email alert receivers can be added in Default receiver emails . These receivers will be available to all users when they create event triggered alerts .","title":"1. Email Alerts"},{"location":"admin/alert/#2-slack-alerts","text":"Alert can also be sent via Slack message. To be able to send Slack messages you first need to configure a Slack webhook. Click on the Configure button on the left side of the slack row and past in your Slack webhook in Webhook . Configure slack Alerts Optionally cluster wide Slack alert receivers can be added in Slack channel/user . These receivers will be available to all users when they create event triggered alerts .","title":"2. Slack Alerts"},{"location":"admin/alert/#3-pagerduty","text":"Pagerduty is another way you can send alerts from Hopsworks. Click on the Configure button on the left side of the pagerduty row and fill out the form that pops up. Configure Pagerduty Alerts Fill in Pagerduty URL: the URL to send API requests to. Optionally cluster wide Pagerduty alert receivers can be added in Service key/Routing key . By first choosing the PagerDuty integration type: global event routing (routing_key) : when using PagerDuty integration type Events API v2 . service (service_key) : when using PagerDuty integration type Prometheus . Then adding the Service key/Routing key of the receiver(s). PagerDuty provides documentation on how to integrate with Prometheus' Alert manager.","title":"3. Pagerduty"},{"location":"admin/alert/#advanced-configuration","text":"If you are familiar with Prometheus' Alert manager you can also configure alerts by editing the yaml/json file directly. Advanced configuration Example: Adding the yaml snippet shown below in the global section of the alert manager configuration will have the same effect as creating the SMTP configuration as shown in section 1 above. global : smtp_smarthost : smtp.gmail.com:587 smtp_from : hopsworks@gmail.com smtp_auth_username : hopsworks@gmail.com smtp_auth_password : XXXXXXXXX smtp_auth_identity : hopsworks@gmail.com ... To test the alerts by creating triggers from Jobs and Feature group validations see Alerts .","title":"Advanced configuration"},{"location":"admin/auth/","text":"Authentication Methods # To configure Authentication methods click on your name in the top right corner of the navigation bar and choose Cluster Settings from the dropdown menu. In the Cluster Settings Authentication tab you can configure how users authenticate. TOTP Two-factor Authentication : can be disabled , optional or mandatory . If set to mandatory all users are required to set up two-factor authentication when registering. Note If two-factor is set to mandatory on a cluster with preexisting users all users will need to go through lost device recovery step to enable two-factor. So consider setting it to optional first and allow users to enable it before setting it to mandatory. OAuth2 : if your organization already have an identity management system compatible with OpenID Connect (OIDC) you can configure Hopsworks to use your identity provider by enabling OAuth as shown in the figure below. After enabling OAuth you can register your identity provider by clicking on Add Identity Provider button. See Create client for details. LDAP/Kerberos : if your organization is using LDAP or Kerberos to manage users and services you can configure Hopsworks to use it as the user management system. You can enable LDAP/Kerberos by clicking on the checkbox, as shown in the figure below, and choosing LDAP or Kerberos. For more information on how to configure LDAP and Kerberos see Configure LDAP and Configure Kerberos . Setup Authentication Methods In the figure above we see a cluster with Two-factor authentication disabled, OAuth enabled with one registered identity provider and LDAP authentication enabled.","title":"Configure Authentication"},{"location":"admin/auth/#authentication-methods","text":"To configure Authentication methods click on your name in the top right corner of the navigation bar and choose Cluster Settings from the dropdown menu. In the Cluster Settings Authentication tab you can configure how users authenticate. TOTP Two-factor Authentication : can be disabled , optional or mandatory . If set to mandatory all users are required to set up two-factor authentication when registering. Note If two-factor is set to mandatory on a cluster with preexisting users all users will need to go through lost device recovery step to enable two-factor. So consider setting it to optional first and allow users to enable it before setting it to mandatory. OAuth2 : if your organization already have an identity management system compatible with OpenID Connect (OIDC) you can configure Hopsworks to use your identity provider by enabling OAuth as shown in the figure below. After enabling OAuth you can register your identity provider by clicking on Add Identity Provider button. See Create client for details. LDAP/Kerberos : if your organization is using LDAP or Kerberos to manage users and services you can configure Hopsworks to use it as the user management system. You can enable LDAP/Kerberos by clicking on the checkbox, as shown in the figure below, and choosing LDAP or Kerberos. For more information on how to configure LDAP and Kerberos see Configure LDAP and Configure Kerberos . Setup Authentication Methods In the figure above we see a cluster with Two-factor authentication disabled, OAuth enabled with one registered identity provider and LDAP authentication enabled.","title":"Authentication Methods"},{"location":"admin/iamRoleChaining/","text":"AWS IAM Role Chaining # Using an EC2 instance profile enables your Hopsworks cluster to access AWS resources. This forces all Hopsworks users to share the instance profile role and the resource access policies attached to that role. To allow for per project access policies you could have your users use AWS credentials directly in their programs which is not recommended so you should instead use Role chaining . To use Role chaining, you need to first setup IAM roles in AWS: Step 1 . Create an instance profile role with policies that will allow it to assume all resource roles that we can assume from the Hopsworks cluster. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AssumeDataRoles\" , \"Effect\" : \"Allow\" , \"Action\" : \"sts:AssumeRole\" , \"Resource\" : [ \"arn:aws:iam::123456789011:role/test-role\" , \"arn:aws:iam::xxxxxxxxxxxx:role/s3-role\" , \"arn:aws:iam::xxxxxxxxxxxx:role/dev-s3-role\" , \"arn:aws:iam::xxxxxxxxxxxx:role/redshift\" ] } ] } Example policy for assuming four roles. Step 2 . Create the resource roles and edit trust relationship and add policy document that will allow the instance profile to assume this role. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"arn:aws:iam::xxxxxxxxxxxx:role/instance-profile\" }, \"Action\" : \"sts:AssumeRole\" } ] } Example policy document. Role chaining allows the instance profile to assume any role in the policy attached in step 1. To limit access to iam roles we can create a per-project mapping from the admin page in Hopsworks. Role Chaining Click on your name in the top right corner of the navigation bar and choose Cluster Settings from the dropdown menu. In the Cluster Settings' IAM Role Chaining tab you can configure the mappings between projects and IAM roles. You can add mappings by entering the project name, which roles in that project can access the cloud role and the role ARN. Optionally you can set a role mapping as default by marking the default checkbox. The default roles can be changed from the project setting by a Data owner in that project. Create Role Chaining Any member of a project can then go to the Project Settings -> Assuming IAM Roles page to see which roles they can assume.","title":"IAM Role Chaining"},{"location":"admin/iamRoleChaining/#aws-iam-role-chaining","text":"Using an EC2 instance profile enables your Hopsworks cluster to access AWS resources. This forces all Hopsworks users to share the instance profile role and the resource access policies attached to that role. To allow for per project access policies you could have your users use AWS credentials directly in their programs which is not recommended so you should instead use Role chaining . To use Role chaining, you need to first setup IAM roles in AWS: Step 1 . Create an instance profile role with policies that will allow it to assume all resource roles that we can assume from the Hopsworks cluster. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AssumeDataRoles\" , \"Effect\" : \"Allow\" , \"Action\" : \"sts:AssumeRole\" , \"Resource\" : [ \"arn:aws:iam::123456789011:role/test-role\" , \"arn:aws:iam::xxxxxxxxxxxx:role/s3-role\" , \"arn:aws:iam::xxxxxxxxxxxx:role/dev-s3-role\" , \"arn:aws:iam::xxxxxxxxxxxx:role/redshift\" ] } ] } Example policy for assuming four roles. Step 2 . Create the resource roles and edit trust relationship and add policy document that will allow the instance profile to assume this role. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"arn:aws:iam::xxxxxxxxxxxx:role/instance-profile\" }, \"Action\" : \"sts:AssumeRole\" } ] } Example policy document. Role chaining allows the instance profile to assume any role in the policy attached in step 1. To limit access to iam roles we can create a per-project mapping from the admin page in Hopsworks. Role Chaining Click on your name in the top right corner of the navigation bar and choose Cluster Settings from the dropdown menu. In the Cluster Settings' IAM Role Chaining tab you can configure the mappings between projects and IAM roles. You can add mappings by entering the project name, which roles in that project can access the cloud role and the role ARN. Optionally you can set a role mapping as default by marking the default checkbox. The default roles can be changed from the project setting by a Data owner in that project. Create Role Chaining Any member of a project can then go to the Project Settings -> Assuming IAM Roles page to see which roles they can assume.","title":"AWS IAM Role Chaining"},{"location":"admin/installation/","text":"Installation notes #","title":"Installation notes"},{"location":"admin/installation/#installation-notes","text":"","title":"Installation notes"},{"location":"admin/project/","text":"Admin Project Guide #","title":"Admin Project Guide"},{"location":"admin/project/#admin-project-guide","text":"","title":"Admin Project Guide"},{"location":"admin/services/","text":"Manage Services # Hopsworks provides administrators with a view of the status/health of the cluster. This information is provided through the Services page. You can find the Services page by clicking on your name, in the top right corner of the navigation bar, and choosing Cluster Settings from the dropdown menu and going to the Services tab. Services page This page give administrators an overview of which services are running on the cluster. It provides information about their status as reported by agents that monitor the status of the different Systemd units. Columns in the services table represent machines in your cluster. Each service running on a machine will have a status running (green) or stopped (red). If a service is not installed on a machine it will have a status not installed (gray). Services are divided into groups, and you can search for a service by its name or group. You can also search for machines by their host name. Services After you find the correct service you will be able to start , stop or restart it, by clicking on its status. Start, Stop and Restart a service Note Stopping some services like the web server (glassfish_domain1) is not recommended. If you stop it you will have to access the machine running the service and start it with systemctl start glassfish_domain1 .","title":"Manage Services"},{"location":"admin/services/#manage-services","text":"Hopsworks provides administrators with a view of the status/health of the cluster. This information is provided through the Services page. You can find the Services page by clicking on your name, in the top right corner of the navigation bar, and choosing Cluster Settings from the dropdown menu and going to the Services tab. Services page This page give administrators an overview of which services are running on the cluster. It provides information about their status as reported by agents that monitor the status of the different Systemd units. Columns in the services table represent machines in your cluster. Each service running on a machine will have a status running (green) or stopped (red). If a service is not installed on a machine it will have a status not installed (gray). Services are divided into groups, and you can search for a service by its name or group. You can also search for machines by their host name. Services After you find the correct service you will be able to start , stop or restart it, by clicking on its status. Start, Stop and Restart a service Note Stopping some services like the web server (glassfish_domain1) is not recommended. If you stop it you will have to access the machine running the service and start it with systemctl start glassfish_domain1 .","title":"Manage Services"},{"location":"admin/user/","text":"User Management # Whether you run Hopsworks on-premise, or on the cloud using hopsworks.ai , you have a Hopsworks cluster which contains all users and projects. Cluster users # All the users of your Hopsworks instance have access to your cluster with different access rights. You can find them by clicking on your name in the top right corner of the navigation bar and choosing Cluster Settings from the dropdown menu and going to the Users tab (You need to have Admin role to get access to the Cluster Settings page). Active Users Cluster roles # Roles let you manage the access rights of a user to the cluster. User: users with this role are only allowed to use the cluster by creating a limited number of projects. Admin: users with this role are allowed to manage the cluster. This includes accepting new users to the cluster or blocking them, managing user quota, configure alerts and setting up authentication methods . Validating and blocking users # By default, a user who register on Hopsworks using their own credentials are not granted access to the cluster. First, a user with an admin role needs to validate their account. By clicking on the Review Requests button you can open a user request review popup as shown in the image below. Review user request On the user request review popup you can activate or block users. Users with a validated email address will have a check mark on their email. Similarly, if a user is no longer allowed access to the cluster you can block them. To keep consistency with the history of your datasets, a user can not be deleted but only blocked. If necessary a user can be deleted manually in the cluster using the command line. You can block a user by clicking on the block icon on the right side of the user in the list. Blocked Users Blocked users will appear on the lower section of the page. Click on display blocked users to show all the blocked users in your cluster. If a user is blocked by mistake you can reactivate it by clicking on the check mark icon that corresponds to that user in the blocked users list. You can also change the role of a user by clicking on the select dropdown that shows the current role of the user. If there are too many users in your cluster, use the search box (available for blocked users too) to filter users by name or email. It is also possible to filter activated users by role. For example to see all administrators in you cluster click on the select dropdown to the right of the search box and choose Admin . Create users # If you want to allow users to login without registering you can pre-create them by clicking on New user . Create new user After setting the user's name and email chose the type of user you want to create (Hopsworks, Kerberos or LDAP). To create a Kerberos or LDAP user you need to get the users UUID from the Kerberos or LDAP server. Hopsworks user can also be assigned a Role . Kerberos and LDAP users on the other hand can only be assigned a role through group mapping. A temporary password will be generated and displayed when you click on Create new user . Copy the password and pass it securely to the user. Copy temporary password Reset user password # In the case where a user loses her/his password and can not recover it with the password recovery , an administrator can reset it for them. On the bottom of the Users page click on the Reset a user password link. A popup window with a dropdown for searching users by name or email will open. Find the user and click on Reset new password . Reset user password A temporary password will be displayed. Copy the password and pass it to the user securely. Copy temporary password A user with a temporary password will see a warning message when going to User settings Authentication tab. Change password Note A temporary password should be changed as soon as possible.","title":"User Management"},{"location":"admin/user/#user-management","text":"Whether you run Hopsworks on-premise, or on the cloud using hopsworks.ai , you have a Hopsworks cluster which contains all users and projects.","title":"User Management"},{"location":"admin/user/#cluster-users","text":"All the users of your Hopsworks instance have access to your cluster with different access rights. You can find them by clicking on your name in the top right corner of the navigation bar and choosing Cluster Settings from the dropdown menu and going to the Users tab (You need to have Admin role to get access to the Cluster Settings page). Active Users","title":"Cluster users"},{"location":"admin/user/#cluster-roles","text":"Roles let you manage the access rights of a user to the cluster. User: users with this role are only allowed to use the cluster by creating a limited number of projects. Admin: users with this role are allowed to manage the cluster. This includes accepting new users to the cluster or blocking them, managing user quota, configure alerts and setting up authentication methods .","title":"Cluster roles"},{"location":"admin/user/#validating-and-blocking-users","text":"By default, a user who register on Hopsworks using their own credentials are not granted access to the cluster. First, a user with an admin role needs to validate their account. By clicking on the Review Requests button you can open a user request review popup as shown in the image below. Review user request On the user request review popup you can activate or block users. Users with a validated email address will have a check mark on their email. Similarly, if a user is no longer allowed access to the cluster you can block them. To keep consistency with the history of your datasets, a user can not be deleted but only blocked. If necessary a user can be deleted manually in the cluster using the command line. You can block a user by clicking on the block icon on the right side of the user in the list. Blocked Users Blocked users will appear on the lower section of the page. Click on display blocked users to show all the blocked users in your cluster. If a user is blocked by mistake you can reactivate it by clicking on the check mark icon that corresponds to that user in the blocked users list. You can also change the role of a user by clicking on the select dropdown that shows the current role of the user. If there are too many users in your cluster, use the search box (available for blocked users too) to filter users by name or email. It is also possible to filter activated users by role. For example to see all administrators in you cluster click on the select dropdown to the right of the search box and choose Admin .","title":"Validating and blocking users"},{"location":"admin/user/#create-users","text":"If you want to allow users to login without registering you can pre-create them by clicking on New user . Create new user After setting the user's name and email chose the type of user you want to create (Hopsworks, Kerberos or LDAP). To create a Kerberos or LDAP user you need to get the users UUID from the Kerberos or LDAP server. Hopsworks user can also be assigned a Role . Kerberos and LDAP users on the other hand can only be assigned a role through group mapping. A temporary password will be generated and displayed when you click on Create new user . Copy the password and pass it securely to the user. Copy temporary password","title":"Create users"},{"location":"admin/user/#reset-user-password","text":"In the case where a user loses her/his password and can not recover it with the password recovery , an administrator can reset it for them. On the bottom of the Users page click on the Reset a user password link. A popup window with a dropdown for searching users by name or email will open. Find the user and click on Reset new password . Reset user password A temporary password will be displayed. Copy the password and pass it to the user securely. Copy temporary password A user with a temporary password will see a warning message when going to User settings Authentication tab. Change password Note A temporary password should be changed as soon as possible.","title":"Reset user password"},{"location":"admin/ha-dr/dr/","text":"Disaster Recovery # Backup # The state of the Hopsworks cluster is divided into data and metadata and distributed across the different node groups. This section of the guide allows you to take a consistent backup between data in the offline and online feature store as well as the metadata. The following services contain critical state that should be backed up: RonDB : as mentioned above, the RonDB is used by Hopsworks to store the cluster metadata as well as the data for the online feature store. HopsFS : HopsFS stores the data for the batch feature store as well as checkpoints and logs for feature engineering applications. Backing up service/application metrics and services/applications logs are out of the scope of this guide. By default metrics and logs are rotated after 7 days. Application logs are available on HopsFS when the application has finished and, as such, are backed up with the rest of HopsFS\u2019 data. Apache Kafka and OpenSearch are additional services maintaining state. The OpenSearch metadata can be reconstructed from the metadata stored on RonDB. Apache Kafka is used in Hopsworks to store the in-flight data that is on its way to the online feature store. In the event of a total loss of the cluster, running jobs with inflight data will have to be replayed. Configuration Backup # Hopsworks adopts an Infrastructure-as-code philosophy, as such all the configuration files for the different Hopsworks services are generated during the deployment phase. Cluster-specific customizations should be centralized in the cluster definition used to deploy the cluster. As such the cluster definition should be backed up (e.g., by committing it to a git repository) to be able to recreate the same cluster in case it needs to be recreated. RonDB Backup # The RonDB backup is divided into two parts: user and privileges backup and data backup. To take the backup of users and privileges you can run the following command from any of the nodes in the head node group. This command generates a SQL file containing all the user definitions for both the metadata services (Hopsworks, HopsFS, Metastore) as well as the user and permission grants for the online feature store. This command needs to be run as user \u2018mysql\u2019 or with sudo privileges. /srv/hops/mysql/bin/mysqlpump -S /srv/hops/mysql-cluster/mysql.sock --exclude-databases = % --exclude-users = root,mysql.sys,mysql.session,mysql.infoschema --users > users.sql The second step is to trigger the backup of the data. This can be achieved by running the following command as user \u2018mysql\u2019 on one of the nodes of the head node group. /srv/hops/mysql-cluster/ndb/scripts/mgm-client.sh -e \"START BACKUP [replace_backup_id] SNAPSHOTEND WAIT COMPLETED\" The backup ID is an integer greater or equal than 1. The script uses the following: $(date +'%y%m%d%H%M') instead of an integer as backup id to make it easier to identify backups over time. The command instructs each RonDB datanode to backup the data it is responsible for. The backup will be located locally on each datanode under the following path: /srv/hops/mysql-cluster/ndb/backups/BACKUP - the directory name will be BACKUP- [ backup_id ] A more comprehensive backup script is available here - The script includes the steps above as well as collecting all the partial RonDB backups on a single node. The script is a good starting point and can be adapted to ship the database backup outside the cluster. HopsFS Backup # HopsFS is a distributed file system based on Apache HDFS. HopsFS stores its metadata in RonDB, as such metadata backup has already been discussed in the section above. The data is stored in the form of blocks on the different data nodes. For availability reasons, the blocks are replicated across three different data nodes. Within a node, the blocks are stored by default under the following directory, under the ownership of the \u2018hdfs\u2019 user: /srv/hopsworks-data/hops/hopsdata/hdfs/dn/ To safely backup all the data, a copy of all the datanodes should be taken. As the data is replicated across the different nodes, excluding a set of nodes might result in data loss. Additionally, as HopsFS blocks are files on the file system and the filesystem can be quite large, the backup is not transactional. Consistency is dictated by the metadata. Blocks being added during the copying process will not be visible when restoring as they are not part of the metadata backup taken prior to cloning the HopsFS blocks. When the HopsFS data blocks are stored in a cloud block storage, for example, Amazon S3, then it is sufficient to only backup the metadata. The blob cloud storage service will ensure durability of the data blocks. Restore # As with the backup phase, the restore operation is broken down in different steps. Cluster deployment # The first step to redeploy the cluster is to redeploy the binaries and configuration. You should reuse the same cluster definition used to deploy the first (original) cluster. This will re-create the same cluster with the same configuration. RonDB restore # The deployment step above created a functioning empty cluster. To restore the cluster, the first step is to restore the metadata and online feature store data stored on RonDB. To restore the state of RonDB, we first need to restore its schemas and tables, then its data, rebuild the indices, and finally restore the users and grants. Restore RonDB schemas and tables # This command should be executed on one of the nodes in the head node group and is going to recreate the schemas, tables, and internal RonDB metadata. In the command below, you should replace the node_id with the id of the node you are running the command on, backup_id with the id of the backup you want to restore. Finally, you should replace the mgm_node_ip with the address of the node where the RonDB management service is running. /srv/hops/mysql/bin/ndb_restore -n [ node_id ] -b [ backup_id ] -m --disable-indexes --ndb-connectstring =[ mgm_node_ip ] :1186 --backup_path = /srv/hops/mysql-cluster/ndb/backups/BACKUP/BACKUP- [ backup_id ] Restore RonDB data # This command should be executed on all the RonDB datanodes. Each command should be customized with the node id of the node you are trying to restore (i.e., replace the node_id). As for the command above you should replace the backup_id and mgm_node_ip. /srv/hops/mysql/bin/ndb_restore -n [ node_id ] -b [ backup_id ] -r --ndb-connectstring =[ mgm_node_ip ] :1186 --backup_path = /srv/hops/mysql-cluster/ndb/backups/BACKUP/BACKUP- [ backup_id ] Rebuild the indices # In the first command we disable the indices for recovery. This last command will take care of enabling them again. This command needs to run only once on one of the nodes of the head node group. As for the commands above, you should replace node_id, backup_id and mgm_node_id. /srv/hops/mysql/bin/ndb_restore -n [ node_id ] -b [ backup_id ] --rebuild-indexes --ndb-connectstring =[ mgm_node_ip ] :1186 --backup_path = /srv/hops/mysql-cluster/ndb/backups/BACKUP/BACKUP- [ backup_ip ] Restore Users and Grants # In the backup phase, we took the backup of the user and grants separately. The last step of the RonDB restore process is to re-create all the users and grants both for Hopsworks services as well as for the online feature store users. This can be achieved by running the following command on one node of the head node group: /srv/hops/mysql-cluster/ndb/scripts/mysql-client.sh source users.sql HopsFS restore # With the metadata restored, you can now proceed to restore the file system blocks on HopsFS and restart the file system. When starting the datanode, it will advertise it\u2019s ID/ClusterID and Storage ID based on the VERSION file that can be found in this directory: /srv/hopsworks-data/hops/hopsdata/hdfs/dn/current It\u2019s important that all the datanodes are restored and they report their block to the namenodes processes running on the head nodes. By default the namenodes in HopsFS will exit \u201cSAFE MODE\u201d (i.e., the mode that allows only read operations) only when the datanodes have reported 99.9% of the blocks the namenodes have in the metadata. As such, the namenodes will not resume operations until all the file blocks have been restored. OpenSearch state rebuild # The OpenSearch state can be rebuilt using the Hopsworks metadata stored on RonDB. The rebuild process is done by using the re-indexing mechanism provided by ePipe. The re-indexing can be triggered by running the following command on the head node where ePipe is running: /srv/hops/epipe/bin/reindex-epipe.sh The script is deployed and configured during the platform deployment. Kafka topics rebuild # The backup and restore plan doesn\u2019t cover the data in transit in Kafka, for which the jobs producing it will have to be replayed. However, the RonDB backup contains the information necessary to recreate the topics of all the feature groups. You can run the following command, as super user, to recreate all the topics with the correct partitioning and replication factors: /srv/hops/kafka/bin/kafka-restore.sh The script is deployed and configured during the platform deployment.","title":"Disaster Recovery"},{"location":"admin/ha-dr/dr/#disaster-recovery","text":"","title":"Disaster Recovery"},{"location":"admin/ha-dr/dr/#backup","text":"The state of the Hopsworks cluster is divided into data and metadata and distributed across the different node groups. This section of the guide allows you to take a consistent backup between data in the offline and online feature store as well as the metadata. The following services contain critical state that should be backed up: RonDB : as mentioned above, the RonDB is used by Hopsworks to store the cluster metadata as well as the data for the online feature store. HopsFS : HopsFS stores the data for the batch feature store as well as checkpoints and logs for feature engineering applications. Backing up service/application metrics and services/applications logs are out of the scope of this guide. By default metrics and logs are rotated after 7 days. Application logs are available on HopsFS when the application has finished and, as such, are backed up with the rest of HopsFS\u2019 data. Apache Kafka and OpenSearch are additional services maintaining state. The OpenSearch metadata can be reconstructed from the metadata stored on RonDB. Apache Kafka is used in Hopsworks to store the in-flight data that is on its way to the online feature store. In the event of a total loss of the cluster, running jobs with inflight data will have to be replayed.","title":"Backup"},{"location":"admin/ha-dr/dr/#configuration-backup","text":"Hopsworks adopts an Infrastructure-as-code philosophy, as such all the configuration files for the different Hopsworks services are generated during the deployment phase. Cluster-specific customizations should be centralized in the cluster definition used to deploy the cluster. As such the cluster definition should be backed up (e.g., by committing it to a git repository) to be able to recreate the same cluster in case it needs to be recreated.","title":"Configuration Backup"},{"location":"admin/ha-dr/dr/#rondb-backup","text":"The RonDB backup is divided into two parts: user and privileges backup and data backup. To take the backup of users and privileges you can run the following command from any of the nodes in the head node group. This command generates a SQL file containing all the user definitions for both the metadata services (Hopsworks, HopsFS, Metastore) as well as the user and permission grants for the online feature store. This command needs to be run as user \u2018mysql\u2019 or with sudo privileges. /srv/hops/mysql/bin/mysqlpump -S /srv/hops/mysql-cluster/mysql.sock --exclude-databases = % --exclude-users = root,mysql.sys,mysql.session,mysql.infoschema --users > users.sql The second step is to trigger the backup of the data. This can be achieved by running the following command as user \u2018mysql\u2019 on one of the nodes of the head node group. /srv/hops/mysql-cluster/ndb/scripts/mgm-client.sh -e \"START BACKUP [replace_backup_id] SNAPSHOTEND WAIT COMPLETED\" The backup ID is an integer greater or equal than 1. The script uses the following: $(date +'%y%m%d%H%M') instead of an integer as backup id to make it easier to identify backups over time. The command instructs each RonDB datanode to backup the data it is responsible for. The backup will be located locally on each datanode under the following path: /srv/hops/mysql-cluster/ndb/backups/BACKUP - the directory name will be BACKUP- [ backup_id ] A more comprehensive backup script is available here - The script includes the steps above as well as collecting all the partial RonDB backups on a single node. The script is a good starting point and can be adapted to ship the database backup outside the cluster.","title":"RonDB Backup"},{"location":"admin/ha-dr/dr/#hopsfs-backup","text":"HopsFS is a distributed file system based on Apache HDFS. HopsFS stores its metadata in RonDB, as such metadata backup has already been discussed in the section above. The data is stored in the form of blocks on the different data nodes. For availability reasons, the blocks are replicated across three different data nodes. Within a node, the blocks are stored by default under the following directory, under the ownership of the \u2018hdfs\u2019 user: /srv/hopsworks-data/hops/hopsdata/hdfs/dn/ To safely backup all the data, a copy of all the datanodes should be taken. As the data is replicated across the different nodes, excluding a set of nodes might result in data loss. Additionally, as HopsFS blocks are files on the file system and the filesystem can be quite large, the backup is not transactional. Consistency is dictated by the metadata. Blocks being added during the copying process will not be visible when restoring as they are not part of the metadata backup taken prior to cloning the HopsFS blocks. When the HopsFS data blocks are stored in a cloud block storage, for example, Amazon S3, then it is sufficient to only backup the metadata. The blob cloud storage service will ensure durability of the data blocks.","title":"HopsFS Backup"},{"location":"admin/ha-dr/dr/#restore","text":"As with the backup phase, the restore operation is broken down in different steps.","title":"Restore"},{"location":"admin/ha-dr/dr/#cluster-deployment","text":"The first step to redeploy the cluster is to redeploy the binaries and configuration. You should reuse the same cluster definition used to deploy the first (original) cluster. This will re-create the same cluster with the same configuration.","title":"Cluster deployment"},{"location":"admin/ha-dr/dr/#rondb-restore","text":"The deployment step above created a functioning empty cluster. To restore the cluster, the first step is to restore the metadata and online feature store data stored on RonDB. To restore the state of RonDB, we first need to restore its schemas and tables, then its data, rebuild the indices, and finally restore the users and grants.","title":"RonDB restore"},{"location":"admin/ha-dr/dr/#restore-rondb-schemas-and-tables","text":"This command should be executed on one of the nodes in the head node group and is going to recreate the schemas, tables, and internal RonDB metadata. In the command below, you should replace the node_id with the id of the node you are running the command on, backup_id with the id of the backup you want to restore. Finally, you should replace the mgm_node_ip with the address of the node where the RonDB management service is running. /srv/hops/mysql/bin/ndb_restore -n [ node_id ] -b [ backup_id ] -m --disable-indexes --ndb-connectstring =[ mgm_node_ip ] :1186 --backup_path = /srv/hops/mysql-cluster/ndb/backups/BACKUP/BACKUP- [ backup_id ]","title":"Restore RonDB schemas and tables"},{"location":"admin/ha-dr/dr/#restore-rondb-data","text":"This command should be executed on all the RonDB datanodes. Each command should be customized with the node id of the node you are trying to restore (i.e., replace the node_id). As for the command above you should replace the backup_id and mgm_node_ip. /srv/hops/mysql/bin/ndb_restore -n [ node_id ] -b [ backup_id ] -r --ndb-connectstring =[ mgm_node_ip ] :1186 --backup_path = /srv/hops/mysql-cluster/ndb/backups/BACKUP/BACKUP- [ backup_id ]","title":"Restore RonDB data"},{"location":"admin/ha-dr/dr/#rebuild-the-indices","text":"In the first command we disable the indices for recovery. This last command will take care of enabling them again. This command needs to run only once on one of the nodes of the head node group. As for the commands above, you should replace node_id, backup_id and mgm_node_id. /srv/hops/mysql/bin/ndb_restore -n [ node_id ] -b [ backup_id ] --rebuild-indexes --ndb-connectstring =[ mgm_node_ip ] :1186 --backup_path = /srv/hops/mysql-cluster/ndb/backups/BACKUP/BACKUP- [ backup_ip ]","title":"Rebuild the indices"},{"location":"admin/ha-dr/dr/#restore-users-and-grants","text":"In the backup phase, we took the backup of the user and grants separately. The last step of the RonDB restore process is to re-create all the users and grants both for Hopsworks services as well as for the online feature store users. This can be achieved by running the following command on one node of the head node group: /srv/hops/mysql-cluster/ndb/scripts/mysql-client.sh source users.sql","title":"Restore Users and Grants"},{"location":"admin/ha-dr/dr/#hopsfs-restore","text":"With the metadata restored, you can now proceed to restore the file system blocks on HopsFS and restart the file system. When starting the datanode, it will advertise it\u2019s ID/ClusterID and Storage ID based on the VERSION file that can be found in this directory: /srv/hopsworks-data/hops/hopsdata/hdfs/dn/current It\u2019s important that all the datanodes are restored and they report their block to the namenodes processes running on the head nodes. By default the namenodes in HopsFS will exit \u201cSAFE MODE\u201d (i.e., the mode that allows only read operations) only when the datanodes have reported 99.9% of the blocks the namenodes have in the metadata. As such, the namenodes will not resume operations until all the file blocks have been restored.","title":"HopsFS restore"},{"location":"admin/ha-dr/dr/#opensearch-state-rebuild","text":"The OpenSearch state can be rebuilt using the Hopsworks metadata stored on RonDB. The rebuild process is done by using the re-indexing mechanism provided by ePipe. The re-indexing can be triggered by running the following command on the head node where ePipe is running: /srv/hops/epipe/bin/reindex-epipe.sh The script is deployed and configured during the platform deployment.","title":"OpenSearch state rebuild"},{"location":"admin/ha-dr/dr/#kafka-topics-rebuild","text":"The backup and restore plan doesn\u2019t cover the data in transit in Kafka, for which the jobs producing it will have to be replayed. However, the RonDB backup contains the information necessary to recreate the topics of all the feature groups. You can run the following command, as super user, to recreate all the topics with the correct partitioning and replication factors: /srv/hops/kafka/bin/kafka-restore.sh The script is deployed and configured during the platform deployment.","title":"Kafka topics rebuild"},{"location":"admin/ha-dr/ha/","text":"High Availability # At a high level a Hopsworks cluster can be divided into 4 groups of nodes. Each node group should be deployed according to the requirements (e.g., 3/5/7 nodes for the head node group) to guarantee the availability of the components. Head nodes : The head node is responsible for running all the metadata, public API, and user interface services that are required for Hopsworks to provide its functionality. They need to be deployed in an odd number (1, 3, 5) as the head nodes run services like Zookeeper and OpenSearch which enforce consistency through quorum based protocols. The head nodes are also responsible for managing the services running on the remaining group of nodes. Worker nodes : The worker node is responsible for executing the feature engineering pipeline code as well as storing the data for the offline feature store (HopsFS). In an on-prem deployment, the data is stored and replicated on the workers\u2019 local hard drives. By default the data is replicated across 3 workers. In a cloud deployment, HopsFS\u2019 data is persisted in a cloud object store (Amazon S3, Azure Blob Storage, Google Cloud Blob Storage) and the HopsFS datanodes are responsible for persisting, retrieving and caching of blocks from the object store. RonDB Data nodes : These nodes are responsible for storing the services\u2019 metadata (Hopsworks, HopsFS, Hive Metastore, Airflow) as well as the data for the online feature store. For high availability, at least two data nodes should be deployed and RonDB is typically configured with a replication factor of 2, as it uses synchronous replication with 2-phase commit, not a quorum-based replication protocol. More advanced deployment patterns and best practices are covered in the RonDB documentation (https://docs.rondb.ai) . Query brokers : The query brokers are the entry point for querying the online feature store. They handle authentication, authorization and execution of the requests for online feature data being submitted from the feature store APIs. At least two query brokers should be deployed to achieve high availability. Query brokers are stateless. Additional query brokers should be deployed to handle additional load and clients. Example deployment: Example High Available deployment For higher availability, a Hopsworks cluster should be deployed across multiple availability zones, however, a single cluster cannot be deployed across multiple regions. Multiple region deployments are out of the scope of this guide. A different service placement is also possible, e.g., separating RonDB data nodes between metadata and online feature store or adding more replicas of a metadata service without necessarily adding a whole new head node, however, this is outside the scope of this guide.","title":"High Availability"},{"location":"admin/ha-dr/ha/#high-availability","text":"At a high level a Hopsworks cluster can be divided into 4 groups of nodes. Each node group should be deployed according to the requirements (e.g., 3/5/7 nodes for the head node group) to guarantee the availability of the components. Head nodes : The head node is responsible for running all the metadata, public API, and user interface services that are required for Hopsworks to provide its functionality. They need to be deployed in an odd number (1, 3, 5) as the head nodes run services like Zookeeper and OpenSearch which enforce consistency through quorum based protocols. The head nodes are also responsible for managing the services running on the remaining group of nodes. Worker nodes : The worker node is responsible for executing the feature engineering pipeline code as well as storing the data for the offline feature store (HopsFS). In an on-prem deployment, the data is stored and replicated on the workers\u2019 local hard drives. By default the data is replicated across 3 workers. In a cloud deployment, HopsFS\u2019 data is persisted in a cloud object store (Amazon S3, Azure Blob Storage, Google Cloud Blob Storage) and the HopsFS datanodes are responsible for persisting, retrieving and caching of blocks from the object store. RonDB Data nodes : These nodes are responsible for storing the services\u2019 metadata (Hopsworks, HopsFS, Hive Metastore, Airflow) as well as the data for the online feature store. For high availability, at least two data nodes should be deployed and RonDB is typically configured with a replication factor of 2, as it uses synchronous replication with 2-phase commit, not a quorum-based replication protocol. More advanced deployment patterns and best practices are covered in the RonDB documentation (https://docs.rondb.ai) . Query brokers : The query brokers are the entry point for querying the online feature store. They handle authentication, authorization and execution of the requests for online feature data being submitted from the feature store APIs. At least two query brokers should be deployed to achieve high availability. Query brokers are stateless. Additional query brokers should be deployed to handle additional load and clients. Example deployment: Example High Available deployment For higher availability, a Hopsworks cluster should be deployed across multiple availability zones, however, a single cluster cannot be deployed across multiple regions. Multiple region deployments are out of the scope of this guide. A different service placement is also possible, e.g., separating RonDB data nodes between metadata and online feature store or adding more replicas of a metadata service without necessarily adding a whole new head node, however, this is outside the scope of this guide.","title":"High Availability"},{"location":"admin/ha-dr/intro/","text":"Hopsworks High Availability and Disaster Recovery Documentation # The Hopsworks Feature Store is the underlying component powering enterprise ML pipelines as well as serving feature data to model making user facing predictions. Sometimes the Hopsworks cluster can experience hardware failures or power loss, to help you plan for these occasions and avoid Hopsworks Feature Store downtime, we put together this guide. This guide is divided into three sections: High availability : deployment patterns and best practices to make sure individual component failures do not impact the availability of the Hopsworks cluster. Backup : configuration policies and best practices to make sure you have have fresh copy of the data and metadata in case of necessity Restore : procedures and best practices to restore a previous backup if needed.","title":"Overview"},{"location":"admin/ha-dr/intro/#hopsworks-high-availability-and-disaster-recovery-documentation","text":"The Hopsworks Feature Store is the underlying component powering enterprise ML pipelines as well as serving feature data to model making user facing predictions. Sometimes the Hopsworks cluster can experience hardware failures or power loss, to help you plan for these occasions and avoid Hopsworks Feature Store downtime, we put together this guide. This guide is divided into three sections: High availability : deployment patterns and best practices to make sure individual component failures do not impact the availability of the Hopsworks cluster. Backup : configuration policies and best practices to make sure you have have fresh copy of the data and metadata in case of necessity Restore : procedures and best practices to restore a previous backup if needed.","title":"Hopsworks High Availability and Disaster Recovery Documentation"},{"location":"admin/ldap/configure-krb/","text":"Configure Kerberos # Kerberos need some server configuration before you can enable it from the UI. For instruction on how to configure your hopsworks server see Server Configuration for Kerberos After configuring the server you can configure Authentication methods by clicking on your name in the top right corner of the navigation bar and choosing Cluster Settings from the dropdown menu. In the Authentication tab you can find in Cluster Settings , you can enable Kerberos by clicking on the Kerberos checkbox. If LDAP/Kerberos checkbox is not checked, make sure that you configured your application server and enable it by clicking on the checkbox. Setup Authentication Methods Finally, click on edit configuration and fill in the attributes. Configure Kerberos Account status: the status a user will be assigned when logging in for the first time. If a user is assigned a status different from Activated an admin needs to manually activate each user from the User management . Group mapping: allows you to specify a mapping between LDAP groups and Hopsworks groups. The mapping is a semicolon separated string in the form Directory Administrators->HOPS_ADMIN;IT People-> HOPS_USER . Default is empty. If no mapping is specified, users need to be assigned a role by an admin before they can log in. User id: the id field in LDAP with a string placeholder. Default uid=%s . User given name: the given name field in LDAP. Default givenName . User surname: the surname field in LDAP. Default sn . User email: the email field in LDAP. Default mail . User search filter: the search filter for user. Default uid=%s . Principal search filter: the search filter for principal name. Default krbPrincipalName=%s . Group search filter: the search filter for groups. Default member=%d . Group target: the target to search for groups in the LDAP directory tree. Default cn . Dynamic group target: the target to search for dynamic groups in the LDAP directory tree. Default memberOf . User dn: specify the distinguished name (DN) of the container or base point where the users are stored. Default is empty. Group dn: specify the DN of the container or base point where the groups are stored. Default is empty. All defaults are taken from OpenLDAP . The login page will now have the choice to use Kerberos for authentication. Log in using Kerberos Note Make sure that you have Kerberos properly configured on your computer and you are logged in. Kerberos support must also be configured on the browser to use Kerberos for authentication.","title":"Configure Kerberos"},{"location":"admin/ldap/configure-krb/#configure-kerberos","text":"Kerberos need some server configuration before you can enable it from the UI. For instruction on how to configure your hopsworks server see Server Configuration for Kerberos After configuring the server you can configure Authentication methods by clicking on your name in the top right corner of the navigation bar and choosing Cluster Settings from the dropdown menu. In the Authentication tab you can find in Cluster Settings , you can enable Kerberos by clicking on the Kerberos checkbox. If LDAP/Kerberos checkbox is not checked, make sure that you configured your application server and enable it by clicking on the checkbox. Setup Authentication Methods Finally, click on edit configuration and fill in the attributes. Configure Kerberos Account status: the status a user will be assigned when logging in for the first time. If a user is assigned a status different from Activated an admin needs to manually activate each user from the User management . Group mapping: allows you to specify a mapping between LDAP groups and Hopsworks groups. The mapping is a semicolon separated string in the form Directory Administrators->HOPS_ADMIN;IT People-> HOPS_USER . Default is empty. If no mapping is specified, users need to be assigned a role by an admin before they can log in. User id: the id field in LDAP with a string placeholder. Default uid=%s . User given name: the given name field in LDAP. Default givenName . User surname: the surname field in LDAP. Default sn . User email: the email field in LDAP. Default mail . User search filter: the search filter for user. Default uid=%s . Principal search filter: the search filter for principal name. Default krbPrincipalName=%s . Group search filter: the search filter for groups. Default member=%d . Group target: the target to search for groups in the LDAP directory tree. Default cn . Dynamic group target: the target to search for dynamic groups in the LDAP directory tree. Default memberOf . User dn: specify the distinguished name (DN) of the container or base point where the users are stored. Default is empty. Group dn: specify the DN of the container or base point where the groups are stored. Default is empty. All defaults are taken from OpenLDAP . The login page will now have the choice to use Kerberos for authentication. Log in using Kerberos Note Make sure that you have Kerberos properly configured on your computer and you are logged in. Kerberos support must also be configured on the browser to use Kerberos for authentication.","title":"Configure Kerberos"},{"location":"admin/ldap/configure-ldap/","text":"Configure LDAP/Kerberos # LDAP need some server configuration before you can enable it from the UI. For instruction on how to configure your hopsworks server see Server Configuration for LDAP After configuring the server you can configure Authentication methods by clicking on your name in the top right corner of the navigation bar and choosing Cluster Settings from the dropdown menu. In the Authentication tab you can find in Cluster Settings , you can enable LDAP by clicking on the LDAP checkbox. If LDAP/Kerberos checkbox is not checked make sure that you configured your application server and enable it by clicking on the checkbox. Setup Authentication Methods Finally, click on edit configuration and fill in the attributes. Configure LDAP Account status: the status a user will be assigned when logging in for the first time. If a use is assigned a status different from Activated an admin needs to manually activate each user from the User management . Group mapping: allows you to specify a mapping between LDAP groups and Hopsworks groups. The mapping is a semicolon separated string in the form Directory Administrators->HOPS_ADMIN;IT People-> HOPS_USER . Default is empty. If no mapping is specified, users need to be assigned a role by an admin before they can log in. User id: the id field in LDAP with a string placeholder. Default uid=%s . User given name: the given name field in LDAP. Default givenName . User surname: the surname field in LDAP. Default sn . User email: the email field in LDAP. Default mail . User search filter: the search filter for user. Default uid=%s . Group search filter: the search filter for groups. Default member=%d . Group target: the target to search for groups in the LDAP directory tree. Default cn . Dynamic group target: the target to search for dynamic groups in the LDAP directory tree. Default memberOf . User dn: specify the distinguished name (DN) of the container or base point where the users are stored. Default is empty. Group dn: specify the DN of the container or base point where the groups are stored. Default is empty. All defaults are taken from OpenLDAP . The login page will now have the choice to use LDAP for authentication. Log in using LDAP","title":"Configure LDAP"},{"location":"admin/ldap/configure-ldap/#configure-ldapkerberos","text":"LDAP need some server configuration before you can enable it from the UI. For instruction on how to configure your hopsworks server see Server Configuration for LDAP After configuring the server you can configure Authentication methods by clicking on your name in the top right corner of the navigation bar and choosing Cluster Settings from the dropdown menu. In the Authentication tab you can find in Cluster Settings , you can enable LDAP by clicking on the LDAP checkbox. If LDAP/Kerberos checkbox is not checked make sure that you configured your application server and enable it by clicking on the checkbox. Setup Authentication Methods Finally, click on edit configuration and fill in the attributes. Configure LDAP Account status: the status a user will be assigned when logging in for the first time. If a use is assigned a status different from Activated an admin needs to manually activate each user from the User management . Group mapping: allows you to specify a mapping between LDAP groups and Hopsworks groups. The mapping is a semicolon separated string in the form Directory Administrators->HOPS_ADMIN;IT People-> HOPS_USER . Default is empty. If no mapping is specified, users need to be assigned a role by an admin before they can log in. User id: the id field in LDAP with a string placeholder. Default uid=%s . User given name: the given name field in LDAP. Default givenName . User surname: the surname field in LDAP. Default sn . User email: the email field in LDAP. Default mail . User search filter: the search filter for user. Default uid=%s . Group search filter: the search filter for groups. Default member=%d . Group target: the target to search for groups in the LDAP directory tree. Default cn . Dynamic group target: the target to search for dynamic groups in the LDAP directory tree. Default memberOf . User dn: specify the distinguished name (DN) of the container or base point where the users are stored. Default is empty. Group dn: specify the DN of the container or base point where the groups are stored. Default is empty. All defaults are taken from OpenLDAP . The login page will now have the choice to use LDAP for authentication. Log in using LDAP","title":"Configure LDAP/Kerberos"},{"location":"admin/ldap/configure-server/","text":"Configure Server for LDAP and Kerberos # LDAP and Kerberos integration need some configuration in the Karamel cluster definition used to deploy your hopsworks cluster. Server Configuration for LDAP # The LDAP attributes below are used to configure JNDI external resource in Payara. The JNDI resource will communicate with your LDAP server to perform the authentication. ldap : enabled : true jndilookupname : \"dc=hopsworks,dc=ai\" provider_url : \"ldap://193.10.66.104:1389\" attr_binary_val : \"entryUUID\" security_auth : \"none\" security_principal : \"\" security_credentials : \"\" referral : \"ignore\" additional_props : \"\" jndilookupname: should contain the LDAP domain. attr_binary_val: is the binary unique identifier that will be used in subsequent logins to identify the user. security_auth: how to authenticate to the LDAP server. security_principal: contains the username of the user that will be used to query LDAP. security_credentials: contains the password of the user that will be used to query LDAP. referral: whether to follow or ignore an alternate location in which an LDAP Request may be processed. Server Configuration for Kerberos # The Kerberos attributes are used to configure SPNEGO . SPNEGO is used to establish a secure context between the requester and the application server when using Kerberos authentication. kerberos : enabled : true krb_conf_path : \"/etc/krb5.conf\" krb_server_key_tab_path : \"/etc/security/keytabs/service.keytab\" krb_server_key_tab_name : \"service.keytab\" spnego_server_conf : '\\nuseKeyTab=true\\nprincipal=\\\"HTTP/server.hopsworks.ai@HOPSWORKS.AI\\\"\\nstoreKey=true\\nisInitiator=false' ldap : jndilookupname : \"dc=hopsworks,dc=ai\" provider_url : \"ldap://193.10.66.104:1389\" attr_binary_val : \"objectGUID\" security_auth : \"none\" security_principal : \"\" security_credentials : \"\" referral : \"ignore\" additional_props : \"\" Both Kerberos and LDAP attributes need to be specified to configure Kerberos. The LDAP attributes are explained above. krb_conf_path: contains the path to the krb5.conf used by SPNEGO to get information about the default domain and the location of the Kerberos KDC. The file is copied by the recipe in to /srv/hops/domains/domain1/config. krb_server_key_tab_path: contains the path to the Kerberos service keytab. The keytab is copied by the recipe in to /srv/hops/domains/domain/config with the name set in the krb_server_key_tab_name attribute. spnego_server_conf: contains the configuration that will be appended to Payara's (application serve used to host hopsworks) login.conf. In particular, it should contain useKeyTab=true, and the principal name to be used in the authentication phase. Initiator should be set to false.","title":"Configure server for LDAP and Kerberos"},{"location":"admin/ldap/configure-server/#configure-server-for-ldap-and-kerberos","text":"LDAP and Kerberos integration need some configuration in the Karamel cluster definition used to deploy your hopsworks cluster.","title":"Configure Server for LDAP and Kerberos"},{"location":"admin/ldap/configure-server/#server-configuration-for-ldap","text":"The LDAP attributes below are used to configure JNDI external resource in Payara. The JNDI resource will communicate with your LDAP server to perform the authentication. ldap : enabled : true jndilookupname : \"dc=hopsworks,dc=ai\" provider_url : \"ldap://193.10.66.104:1389\" attr_binary_val : \"entryUUID\" security_auth : \"none\" security_principal : \"\" security_credentials : \"\" referral : \"ignore\" additional_props : \"\" jndilookupname: should contain the LDAP domain. attr_binary_val: is the binary unique identifier that will be used in subsequent logins to identify the user. security_auth: how to authenticate to the LDAP server. security_principal: contains the username of the user that will be used to query LDAP. security_credentials: contains the password of the user that will be used to query LDAP. referral: whether to follow or ignore an alternate location in which an LDAP Request may be processed.","title":"Server Configuration for LDAP"},{"location":"admin/ldap/configure-server/#server-configuration-for-kerberos","text":"The Kerberos attributes are used to configure SPNEGO . SPNEGO is used to establish a secure context between the requester and the application server when using Kerberos authentication. kerberos : enabled : true krb_conf_path : \"/etc/krb5.conf\" krb_server_key_tab_path : \"/etc/security/keytabs/service.keytab\" krb_server_key_tab_name : \"service.keytab\" spnego_server_conf : '\\nuseKeyTab=true\\nprincipal=\\\"HTTP/server.hopsworks.ai@HOPSWORKS.AI\\\"\\nstoreKey=true\\nisInitiator=false' ldap : jndilookupname : \"dc=hopsworks,dc=ai\" provider_url : \"ldap://193.10.66.104:1389\" attr_binary_val : \"objectGUID\" security_auth : \"none\" security_principal : \"\" security_credentials : \"\" referral : \"ignore\" additional_props : \"\" Both Kerberos and LDAP attributes need to be specified to configure Kerberos. The LDAP attributes are explained above. krb_conf_path: contains the path to the krb5.conf used by SPNEGO to get information about the default domain and the location of the Kerberos KDC. The file is copied by the recipe in to /srv/hops/domains/domain1/config. krb_server_key_tab_path: contains the path to the Kerberos service keytab. The keytab is copied by the recipe in to /srv/hops/domains/domain/config with the name set in the krb_server_key_tab_name attribute. spnego_server_conf: contains the configuration that will be appended to Payara's (application serve used to host hopsworks) login.conf. In particular, it should contain useKeyTab=true, and the principal name to be used in the authentication phase. Initiator should be set to false.","title":"Server Configuration for Kerberos"},{"location":"admin/oauth2/create-azure-client/","text":"Create An Application in Azure Active Directory. # This example uses Azure Active Directory as the identity provider, but the same can be done with any identity provider supporting OAuth2. Configure your identity provider. # To use OAuth2 in hopsworks you first need to create and configure an OAuth client in your identity provider. We will take the example of Azure AD for the remaining of this documentation, but equivalent steps can be taken on other identity providers. Navigate to the Microsoft Azure Portal and authenticate. Navigate to Azure Active Directory . Click on App Registrations . Click on New Registration . Create application Enter a name for the client such as hopsworks_oauth_client . Verify the Supported account type is set to Accounts in this organizational directory only . And Click Register. Name application In the Overview section, copy the Application (client) ID field . We will use it in Identity Provider registration under the name Client id . Copy client ID Click on Endpoints and copy the OpenId Connect metadata document endpoint excluding the .well-known/openid-configuration part. We will use it in Identity Provider registration under the name Connection URL . Endpoint Click on Certificates & secrets , then Click on New client secret . New client secret Add a description of the secret. Select an expiration period. And, Click Add . Client secret creation Copy the secret. This will be used in Identity Provider registration under the name Client Secret . Client secret creation Click on Authentication . Then click on Add a platform Add a platform In Configure platforms click on Web . Configure platform: Web Enter the Redirect URI and click on Configure . The redirect URI is HOPSWORKS-URI/callback with HOPSWORKS-URI the URI of your hopsworks cluster. Configure platform: Redirect Note If your hopsworks cluster is created on the cloud (hopsworks.ai), you can find your HOPSWORKS-URI by going to the hopsworks.ai dashboard in the General tab of your cluster and copying the URI.","title":"Create Azure Client"},{"location":"admin/oauth2/create-azure-client/#create-an-application-in-azure-active-directory","text":"This example uses Azure Active Directory as the identity provider, but the same can be done with any identity provider supporting OAuth2.","title":"Create An Application in Azure Active Directory."},{"location":"admin/oauth2/create-azure-client/#configure-your-identity-provider","text":"To use OAuth2 in hopsworks you first need to create and configure an OAuth client in your identity provider. We will take the example of Azure AD for the remaining of this documentation, but equivalent steps can be taken on other identity providers. Navigate to the Microsoft Azure Portal and authenticate. Navigate to Azure Active Directory . Click on App Registrations . Click on New Registration . Create application Enter a name for the client such as hopsworks_oauth_client . Verify the Supported account type is set to Accounts in this organizational directory only . And Click Register. Name application In the Overview section, copy the Application (client) ID field . We will use it in Identity Provider registration under the name Client id . Copy client ID Click on Endpoints and copy the OpenId Connect metadata document endpoint excluding the .well-known/openid-configuration part. We will use it in Identity Provider registration under the name Connection URL . Endpoint Click on Certificates & secrets , then Click on New client secret . New client secret Add a description of the secret. Select an expiration period. And, Click Add . Client secret creation Copy the secret. This will be used in Identity Provider registration under the name Client Secret . Client secret creation Click on Authentication . Then click on Add a platform Add a platform In Configure platforms click on Web . Configure platform: Web Enter the Redirect URI and click on Configure . The redirect URI is HOPSWORKS-URI/callback with HOPSWORKS-URI the URI of your hopsworks cluster. Configure platform: Redirect Note If your hopsworks cluster is created on the cloud (hopsworks.ai), you can find your HOPSWORKS-URI by going to the hopsworks.ai dashboard in the General tab of your cluster and copying the URI.","title":"Configure your identity provider."},{"location":"admin/oauth2/create-client/","text":"Register Identity Provider in Hopsworks # Before registering your identity provider in Hopsworks you need to create a client application in your identity provider and acquire a client id and a client secret . An example on how to create a client using Okta and Azure Active Directory identity providers can be found here and here respectively. After acquiring the client id and client secret create the client in Hopsworks by enabling OAuth2 and clicking on add another identity provider in the Authentication configuration page . Then set base uri of your identity provider in Connection URL give a name to your identity provider (the name will be used in the login page as an alternative login method) and set the client id and client secret in their respective fields, as shown in the figure below. Application overview Connection URL : (provider Uri) is the base uri of the identity provider's API (URI should contain scheme http:// or https://). Additional configuration can be set here: Verify email : if checked only users with verified email address (in the identity provider) can log in to Hopsworks. Code challenge : if your identity provider requires code challenge for authorization request check the code challenge check box. This will allow you to choose code challenge method that can be either plain or S256 . Logo URL : optionally a logo URL to an image can be added. The logo will be shown on the login page with the name as shown in the figure below. Group mapping # Optionally you can add a group mapping from your identity provider to hopsworks groups, by clicking on your name in the top right corner of the navigation bar and choosing Cluster Settings from the dropdown menu. In the Cluster Settings Configuration tab search for oauth_group_mapping and click on the edit button. Set Configuration variables Note Setting oauth_group_mapping to ANY_GROUP->HOPS_USER will assign the role user to any user from any group in your identity provider when they log into hopsworks with OAuth for the first time. You can replace ANY_GROUP with the group of your choice in the identity provider. You can replace HOPS_USER by HOPS_ADMIN if you want the users of that group to be admins in hopsworks. You can do several mappings by separating them with a semicolon. Users will now see a new button on the login page. The button has the name you set above for Name and will redirect to your identity provider. Login with OAuth2 Note When creating a client make sure you can access the provider metadata by making a GET request on the well known endpoint of the provider. The well-known URL, will typically be the Connection URL plus .well-known/openid-configuration . For the above client it would be https://dev-86723251.okta.com/.well-known/openid-configuration .","title":"Register an Identity Provider"},{"location":"admin/oauth2/create-client/#register-identity-provider-in-hopsworks","text":"Before registering your identity provider in Hopsworks you need to create a client application in your identity provider and acquire a client id and a client secret . An example on how to create a client using Okta and Azure Active Directory identity providers can be found here and here respectively. After acquiring the client id and client secret create the client in Hopsworks by enabling OAuth2 and clicking on add another identity provider in the Authentication configuration page . Then set base uri of your identity provider in Connection URL give a name to your identity provider (the name will be used in the login page as an alternative login method) and set the client id and client secret in their respective fields, as shown in the figure below. Application overview Connection URL : (provider Uri) is the base uri of the identity provider's API (URI should contain scheme http:// or https://). Additional configuration can be set here: Verify email : if checked only users with verified email address (in the identity provider) can log in to Hopsworks. Code challenge : if your identity provider requires code challenge for authorization request check the code challenge check box. This will allow you to choose code challenge method that can be either plain or S256 . Logo URL : optionally a logo URL to an image can be added. The logo will be shown on the login page with the name as shown in the figure below.","title":"Register Identity Provider in Hopsworks"},{"location":"admin/oauth2/create-client/#group-mapping","text":"Optionally you can add a group mapping from your identity provider to hopsworks groups, by clicking on your name in the top right corner of the navigation bar and choosing Cluster Settings from the dropdown menu. In the Cluster Settings Configuration tab search for oauth_group_mapping and click on the edit button. Set Configuration variables Note Setting oauth_group_mapping to ANY_GROUP->HOPS_USER will assign the role user to any user from any group in your identity provider when they log into hopsworks with OAuth for the first time. You can replace ANY_GROUP with the group of your choice in the identity provider. You can replace HOPS_USER by HOPS_ADMIN if you want the users of that group to be admins in hopsworks. You can do several mappings by separating them with a semicolon. Users will now see a new button on the login page. The button has the name you set above for Name and will redirect to your identity provider. Login with OAuth2 Note When creating a client make sure you can access the provider metadata by making a GET request on the well known endpoint of the provider. The well-known URL, will typically be the Connection URL plus .well-known/openid-configuration . For the above client it would be https://dev-86723251.okta.com/.well-known/openid-configuration .","title":"Group mapping"},{"location":"admin/oauth2/create-okta-client/","text":"Create An Application in Okta # This example uses an Okta development account to create an application that will represent a Hopsworks client in the identity provider. To create a developer account go to Okta developer . After creating a developer account register a client by going to Applications and click on Create App Integration . Okta Applications This will open a popup as shown in the figure below. Select OIDC as Sign-in-method and Web Application as Application type and click next. Create new Application Give your application a name and select Client credential as Grant Type . Then add a Sign-in redirect URI that is your Hopsworks cluster domain name (including the port number if needed) with path /callback , and a Sign-out redirect URI that is Hopsworks cluster domain name (including the port number if needed) with no path. New Application If you want to limit who can access your Hopsworks cluster select Limit access to selected groups and select group(s) you want to give access to. Here we will allow everyone in the organization to access the cluster. Group assignment Group mapping # You can also create mappings from groups in Okta to groups in Hopsworks. To achieve this you need to configure Okta to send Groups with user information. To do this go to Applications and select your application name. In the Sign On tab click edit OpenID Connect ID Token and select Filter for Groups claim type , then for Groups claim filter add groups as the claim name, select Match Regex from the dropdown and .* (dot star) as Regex to match all groups. See Group mapping on how to do the mapping in Hopsworks. Group claim After the application is created go back to Applications and click on the application you just created. Use the Okta domain ( Connection URL ), client id and client secret generated for your app in the Identity Provider registration in Hopsworks. Application overview Note When copying the domain in the figure above make sure to add the url scheme (http:// or https://) when using it in the Connection URL in the Identity Provider registration form .","title":"Create Okta Client"},{"location":"admin/oauth2/create-okta-client/#create-an-application-in-okta","text":"This example uses an Okta development account to create an application that will represent a Hopsworks client in the identity provider. To create a developer account go to Okta developer . After creating a developer account register a client by going to Applications and click on Create App Integration . Okta Applications This will open a popup as shown in the figure below. Select OIDC as Sign-in-method and Web Application as Application type and click next. Create new Application Give your application a name and select Client credential as Grant Type . Then add a Sign-in redirect URI that is your Hopsworks cluster domain name (including the port number if needed) with path /callback , and a Sign-out redirect URI that is Hopsworks cluster domain name (including the port number if needed) with no path. New Application If you want to limit who can access your Hopsworks cluster select Limit access to selected groups and select group(s) you want to give access to. Here we will allow everyone in the organization to access the cluster. Group assignment","title":"Create An Application in Okta"},{"location":"admin/oauth2/create-okta-client/#group-mapping","text":"You can also create mappings from groups in Okta to groups in Hopsworks. To achieve this you need to configure Okta to send Groups with user information. To do this go to Applications and select your application name. In the Sign On tab click edit OpenID Connect ID Token and select Filter for Groups claim type , then for Groups claim filter add groups as the claim name, select Match Regex from the dropdown and .* (dot star) as Regex to match all groups. See Group mapping on how to do the mapping in Hopsworks. Group claim After the application is created go back to Applications and click on the application you just created. Use the Okta domain ( Connection URL ), client id and client secret generated for your app in the Identity Provider registration in Hopsworks. Application overview Note When copying the domain in the figure above make sure to add the url scheme (http:// or https://) when using it in the Connection URL in the Identity Provider registration form .","title":"Group mapping"},{"location":"compute/","text":"Introduction # Hopsworks and its Feature Store are an open source data-intensive AI platform used for the development and operation of machine learning models at scale. Hopsworks architecture","title":"Introduction"},{"location":"compute/#introduction","text":"Hopsworks and its Feature Store are an open source data-intensive AI platform used for the development and operation of machine learning models at scale. Hopsworks architecture","title":"Introduction"},{"location":"compute/alerts/","text":"Alerts # Alerts can be triggered by Jobs or Feature group validations. This can be achieved by creating alert for a job, a feature group or a project. Project alerts ( Global Alerts ) can be triggered by any job or feature group validation in that project. Whereas alerts created for a job ( Job Alerts ) or a feature group ( Feature validation Alerts ) are only triggered by the specific job or feature group they are created for. When an alert is triggered it sends a message, via the chosen channel (email, slack or pagerduty), to a user or group of users referred here as Receivers . Create Alert Receivers # Receivers are the destinations to which your alerts will be sent to. To create Alert Receivers go a project you want to send alerts for and click on Settings on the side menu. In the submenu you will find Alerts , click on it to go to the alerts page. In the alerts page you can see if alert channels (email, slack or pagerduty) are configured for the cluster. If not go to the Alert Configuration to see how you can configure them. If the alert channels you want to use are configured you can create a receiver for those channels by clicking on Add receiver . Receivers can also be created by an admin when configuring channels in Alert Configuration . Alert receivers Channel: the channel through which the receiver will get alerts. Receiver name: a unique name that will represent the receiver(s). Slack channels/users: channel names or users. This filed will be Emails if the channel chosen is email and service key or routing key if pagerduty is the channel. Note Receivers created inside a project can only be used by alerts created in that project. Receivers created by an admin on the other hand can be used by any alert created in the cluster. Global Alerts # To create a Global Alert you need a channel and at least one receiver. See the section above on how to Create Alert Receivers . A Global Alert is created for a project and can be configured to trigger on any job or feature validation event in that project. You can create a Global Alert by going to your project setting\u2019s alerts section and clicking on Add global event . Project alerts Trigger: the event that will trigger the alert. Alerts can be triggered on events: on job success, fail or killed on data ingestion success, warning or fail Receiver: the receiver that will be notified when an alert is triggered. Severity: the potential severity this alert should report. Can be info, warning or critical. Feature validation Alerts # You can create alerts on a per-feature-group basis if you want to receive notifications for feature group specific events. Before you proceed make sure you have created receivers (see section Create Alert Receivers on how to do that). Feature validation Alerts can be created when a feature group is created or from the edit feature group page for existing feature groups. When creating a new Feature group you will find the Alerts card on the bottom of the Create New Feature Group page. From the Alerts card you can add alerts by clicking on Add another alert and choose the trigger, severity and receiver as explained in the section above. Create Feature Validation Alert for New Feature group To create alerts for an existing feature group go to the feature group overview page (by clicking on the magnifying glass icon). On the bottom of the page you will find the Alerts card. Click on Add an alert and choose the trigger, receiver and severity as explained in the Global Alerts section above. Create Feature Validation Alert for an Existing Feature Group From here you can also edit existing alerts, test the alerts or delete them. Click on the pen icon to edit the severity or the receivers of an alert. The play icon will let you send a test alert to the receivers of that alert. Feature group specific alerts override any Global Alert in the project that will trigger for the same event type. Job Alerts # To create a job alert see Job Alert .","title":"Alerts"},{"location":"compute/alerts/#alerts","text":"Alerts can be triggered by Jobs or Feature group validations. This can be achieved by creating alert for a job, a feature group or a project. Project alerts ( Global Alerts ) can be triggered by any job or feature group validation in that project. Whereas alerts created for a job ( Job Alerts ) or a feature group ( Feature validation Alerts ) are only triggered by the specific job or feature group they are created for. When an alert is triggered it sends a message, via the chosen channel (email, slack or pagerduty), to a user or group of users referred here as Receivers .","title":"Alerts"},{"location":"compute/alerts/#create-alert-receivers","text":"Receivers are the destinations to which your alerts will be sent to. To create Alert Receivers go a project you want to send alerts for and click on Settings on the side menu. In the submenu you will find Alerts , click on it to go to the alerts page. In the alerts page you can see if alert channels (email, slack or pagerduty) are configured for the cluster. If not go to the Alert Configuration to see how you can configure them. If the alert channels you want to use are configured you can create a receiver for those channels by clicking on Add receiver . Receivers can also be created by an admin when configuring channels in Alert Configuration . Alert receivers Channel: the channel through which the receiver will get alerts. Receiver name: a unique name that will represent the receiver(s). Slack channels/users: channel names or users. This filed will be Emails if the channel chosen is email and service key or routing key if pagerduty is the channel. Note Receivers created inside a project can only be used by alerts created in that project. Receivers created by an admin on the other hand can be used by any alert created in the cluster.","title":"Create Alert Receivers"},{"location":"compute/alerts/#global-alerts","text":"To create a Global Alert you need a channel and at least one receiver. See the section above on how to Create Alert Receivers . A Global Alert is created for a project and can be configured to trigger on any job or feature validation event in that project. You can create a Global Alert by going to your project setting\u2019s alerts section and clicking on Add global event . Project alerts Trigger: the event that will trigger the alert. Alerts can be triggered on events: on job success, fail or killed on data ingestion success, warning or fail Receiver: the receiver that will be notified when an alert is triggered. Severity: the potential severity this alert should report. Can be info, warning or critical.","title":"Global Alerts"},{"location":"compute/alerts/#feature-validation-alerts","text":"You can create alerts on a per-feature-group basis if you want to receive notifications for feature group specific events. Before you proceed make sure you have created receivers (see section Create Alert Receivers on how to do that). Feature validation Alerts can be created when a feature group is created or from the edit feature group page for existing feature groups. When creating a new Feature group you will find the Alerts card on the bottom of the Create New Feature Group page. From the Alerts card you can add alerts by clicking on Add another alert and choose the trigger, severity and receiver as explained in the section above. Create Feature Validation Alert for New Feature group To create alerts for an existing feature group go to the feature group overview page (by clicking on the magnifying glass icon). On the bottom of the page you will find the Alerts card. Click on Add an alert and choose the trigger, receiver and severity as explained in the Global Alerts section above. Create Feature Validation Alert for an Existing Feature Group From here you can also edit existing alerts, test the alerts or delete them. Click on the pen icon to edit the severity or the receivers of an alert. The play icon will let you send a test alert to the receivers of that alert. Feature group specific alerts override any Global Alert in the project that will trigger for the same event type.","title":"Feature validation Alerts"},{"location":"compute/alerts/#job-alerts","text":"To create a job alert see Job Alert .","title":"Job Alerts"},{"location":"compute/git/","text":"Git integration # The Git integration allows users to manage and version control their codebase and workflows using Git directly from the Hopsworks UI. The integration currently supports repositories hosted on Github, Gitlab and Bitbucket. Beta The feature is currently in Beta and will be improved in the upcoming releases. Credentials # When you perform Git operations on Hopsworks that need to interact with the remote repository, Hopsworks relies on the Git HTTPS protocol to perform those operations. Authentication with the remote repository happens through a token generated by the Git repository hosting service (Github, Gitlab, Bitbucket). Documentation on how to generate a token for the supported Git hosting services is available here: Github Gitlab Bitbucket Once the token has been generated, you need to provide it to Hopsworks so that it can be used to authenticate Git operations. In the Account Settings page you can find the Git Providers section. The Git provider section displays which providers have been already configured and can be used to clone new repositories. Git provider configuration list You can click on the Edit Configuration to change a provider username or token, or to configure a new provider. Git provider configuration Tick the checkbox next to the provider you want to configure and insert the username and the token to use for that provider. Tokens are personal The tokens are personal to each user. When you perform operations on a repository, your token is going to be used, even though the repository might belong to a different user. Repositories # Repositories are cloned and managed within the scope of a project. The content of the repository will reside on the Hopsworks File System. The content of the repository can be edited from Jupyter notebooks and can for example be used to configure Jobs. Repositories can be managed from the Git section in the project settings. The Git overview in the project settings provides a list of repositories currently cloned within the project, the location of their content as well which branch and commit their HEAD is currently at. The list of past and ongoing activities on the repositories is also displayed. The list contains the repository on which the activity is performed, the user performing the operation, its status as well as a message describing the outcome. Git provider configuration Clone a repository # To clone a new repository, click on the Clone repository button on the Git overview page. Git clone The clone page asks you to specify the URL of the repository you want to clone. As mentioned above, the supported protocol is HTTPS. As an example, if the repository is hosted on Github, the URL should look like: https://github.com/logicalclocks/hopsworks.git . Additionally the UI asks you to specify which branch you want to clone. By default the UI is going to clone the main branch, however a different branch or commit can be specified by selecting clone from a specific branch . You can select the folder, within your project, on which the repository should be cloned. By default, the repository is going to be cloned within the Resources dataset. However, by clicking on the location button, a different location can be selected. Finally, click on the Clone repository button to trigger the cloning of the repository. Repository actions # On each repository a set of actions can be performed. Git actions Pull latest changes # You can pull changes from the remote repository into your Hopsworks local branch. If you need to resolve conflicts, you can use the Jupyter notebook to do so, or you can do that externally from your local machine. Switch branch # The Switch branch action allows you to change the current branch of the repository. You can either provide a branch name or a commit name you want to checkout. If you tick the Create new option, a new branch will be created on the Hopsworks local repository. This is useful if you are developing a new feature. Switch branch Commit # The commit action allows you to commit changes you made to the Git repository cloned on Hopsworks. To be able to commit, you need to provide a commit message. Commit Push # The push action allows you to push the local changes to the remote repository. The changes will be pushed to the same remote branch as the one you have checked out locally on Hopsworks. Delete # The delete action allows you to remove the local repository from Hopsworks. Potential data loss When you remove a repository, the data/code is also removed from the Hopsworks file system. Use this option carefully or you might lose any non-committed changes or changes that have not been pushed to the remote repository. Repository history # For every repository, the metadata of the most recent 20 commits is available in Hopsworks. The history is available by clicking on the History button in the repository overview page. This allows you to compare the history of the local repository with the list of commits available on the remote repository. Git commit history","title":"Git Integration"},{"location":"compute/git/#git-integration","text":"The Git integration allows users to manage and version control their codebase and workflows using Git directly from the Hopsworks UI. The integration currently supports repositories hosted on Github, Gitlab and Bitbucket. Beta The feature is currently in Beta and will be improved in the upcoming releases.","title":"Git integration"},{"location":"compute/git/#credentials","text":"When you perform Git operations on Hopsworks that need to interact with the remote repository, Hopsworks relies on the Git HTTPS protocol to perform those operations. Authentication with the remote repository happens through a token generated by the Git repository hosting service (Github, Gitlab, Bitbucket). Documentation on how to generate a token for the supported Git hosting services is available here: Github Gitlab Bitbucket Once the token has been generated, you need to provide it to Hopsworks so that it can be used to authenticate Git operations. In the Account Settings page you can find the Git Providers section. The Git provider section displays which providers have been already configured and can be used to clone new repositories. Git provider configuration list You can click on the Edit Configuration to change a provider username or token, or to configure a new provider. Git provider configuration Tick the checkbox next to the provider you want to configure and insert the username and the token to use for that provider. Tokens are personal The tokens are personal to each user. When you perform operations on a repository, your token is going to be used, even though the repository might belong to a different user.","title":"Credentials"},{"location":"compute/git/#repositories","text":"Repositories are cloned and managed within the scope of a project. The content of the repository will reside on the Hopsworks File System. The content of the repository can be edited from Jupyter notebooks and can for example be used to configure Jobs. Repositories can be managed from the Git section in the project settings. The Git overview in the project settings provides a list of repositories currently cloned within the project, the location of their content as well which branch and commit their HEAD is currently at. The list of past and ongoing activities on the repositories is also displayed. The list contains the repository on which the activity is performed, the user performing the operation, its status as well as a message describing the outcome. Git provider configuration","title":"Repositories"},{"location":"compute/git/#clone-a-repository","text":"To clone a new repository, click on the Clone repository button on the Git overview page. Git clone The clone page asks you to specify the URL of the repository you want to clone. As mentioned above, the supported protocol is HTTPS. As an example, if the repository is hosted on Github, the URL should look like: https://github.com/logicalclocks/hopsworks.git . Additionally the UI asks you to specify which branch you want to clone. By default the UI is going to clone the main branch, however a different branch or commit can be specified by selecting clone from a specific branch . You can select the folder, within your project, on which the repository should be cloned. By default, the repository is going to be cloned within the Resources dataset. However, by clicking on the location button, a different location can be selected. Finally, click on the Clone repository button to trigger the cloning of the repository.","title":"Clone a repository"},{"location":"compute/git/#repository-actions","text":"On each repository a set of actions can be performed. Git actions","title":"Repository actions"},{"location":"compute/git/#pull-latest-changes","text":"You can pull changes from the remote repository into your Hopsworks local branch. If you need to resolve conflicts, you can use the Jupyter notebook to do so, or you can do that externally from your local machine.","title":"Pull latest changes"},{"location":"compute/git/#switch-branch","text":"The Switch branch action allows you to change the current branch of the repository. You can either provide a branch name or a commit name you want to checkout. If you tick the Create new option, a new branch will be created on the Hopsworks local repository. This is useful if you are developing a new feature. Switch branch","title":"Switch branch"},{"location":"compute/git/#commit","text":"The commit action allows you to commit changes you made to the Git repository cloned on Hopsworks. To be able to commit, you need to provide a commit message. Commit","title":"Commit"},{"location":"compute/git/#push","text":"The push action allows you to push the local changes to the remote repository. The changes will be pushed to the same remote branch as the one you have checked out locally on Hopsworks.","title":"Push"},{"location":"compute/git/#delete","text":"The delete action allows you to remove the local repository from Hopsworks. Potential data loss When you remove a repository, the data/code is also removed from the Hopsworks file system. Use this option carefully or you might lose any non-committed changes or changes that have not been pushed to the remote repository.","title":"Delete"},{"location":"compute/git/#repository-history","text":"For every repository, the metadata of the most recent 20 commits is available in Hopsworks. The history is available by clicking on the History button in the repository overview page. This allows you to compare the history of the local repository with the list of commits available on the remote repository. Git commit history","title":"Repository history"},{"location":"compute/jobs/","text":"Jobs # All members of a project in Hopsworks can launch the following types of applications through a project's Jobs service: Python ( Hopsworks Enterprise only ) Apache Spark Apache Flink Docker ( Hopsworks Enterprise only ) Launching a job of any type is very similar process, what mostly differs between job types is the various configuration parameters each job type comes with. Manage jobs # You can quickly create a new job by following these steps: From the project menu click Jobs and then the New job button Select a file to be used as the program the job will execute. The file can be selected either from within the project, that means you need to select a file stored in one of the project's datasets, or select a file from your local workstation which will be uploaded to the Resources dataset by default. You can also select Jupyter notebooks as files to be executed. When running a job, Hopsworks will automatically convert the PySpark/Python notebook to a .py file and run it. The notebook is converted every time the job runs, which means changes in the notebook will be picked up by the job without having to update it. Fill in a name for the job. The name can be made of alphanumeric characters, dash or underscore. A job's name is unique across the project, that is no two jobs can have the same name. If you would like to explore more advanced options such as importing a job or setting alerts, then you need to click the Advanced options button shown in the image below and from that page you can configure the options shown in the sections below. New job quickstart New job from scratch # In addition to the options available with the new job quickstart, from this menu you can set default arguments to be given as input to the job when executed. You can overwrite these arguments when running a job by providing input arguments for that particular execution of the job. For example, in the image below the default arguments \"a b c\" will be passed as space-separated arguments to the job. Job default arguments Import an existing job # Instead of creating a new job from scratch, you can create one based on another jobs' configuration. The latter can be exported by using the Export button from the job's Overview page as shown below. This action will download a .json file containing the job's specification which you can then import from the advanced options page. Job export Compute Configuration # You can set a default configuration for each job type (currently only supported for Spark and Python ) at a project scope, from the Project settings - Compute configuration menu option. That enables you to have the job configuration parameters already filled in when creating a job and optionally override them for this particular job instance. Compute configuration page Spark # The image below shows an example of a Spark job advanced configuration page. If set, these configuration properties will override the default project-scoped job configuration properties described in the section above. You can set the following properties for Spark/PySpark jobs: Main class: For Spark jobs, the main class of the application Driver memory Driver virtual cores Executor memory Executor virtual cores Dynamic - Static: Run the Spark job in static or dynamic allocation mode (see official docs for details). Additional archives: List of zip or .tgz files that will be locally accessible by the application Additional jars: List of .jar files to add to the CLASSPATH of the application Additional Python dependencies: List of .py, .zip or .egg files that will be locally accessible by the application Additional files: List of files that will be locally accessible by the application Properties: Optional line-separates properties to be set for the Spark application. For example, to set environment variables for the driver you can set the properties as shown below spark.yarn.appMasterEnv.envvar=value spark.yarn.appMasterEnv.envvar2=value2 Spark job configuration properties If you are creating the job programmatically, the following JSON is an example job configuration for a PYSPARK job: sparkJobConfiguration.json { \"type\" : \"sparkJobConfiguration\" , \"amQueue\" : \"default\" , // Should be set to \"default\", represents name of YARN queue \"amMemory\" : 2048 , // Memory in megabytes to configure for Spark driver \"amVCores\" : 1 , // Virtual cores to configure for Spark driver \"jobType\" : \"PYSPARK\" , // Should be SPARK if application is a .jar file, otherwise PYSPARK if .py or .ipynb is configured \"appPath\" : \"hdfs:///Projects/{PROJECT_NAME}/{DATASET_NAME}/{APP_FILE}\" , // Path to application file to execute, supported files are .jar, .py or .ipynb \"mainClass\" : \"org.apache.spark.deploy.PythonRunner\" , // Must be set to \"org.apache.spark.deploy.PythonRunner\" for PYSPARK jobs, otherwise Main class of configured jar to run. \"spark.executor.instances\" : 1 , \"spark.executor.cores\" : 1 , \"spark.executor.memory\" : 2048 , \"spark.executor.gpus\" : 0 , \"spark.tensorflow.num.ps\" : 1 , \"spark.dynamicAllocation.enabled\" : true , \"spark.dynamicAllocation.minExecutors\" : 0 , \"spark.dynamicAllocation.maxExecutors\" : 1 , \"spark.dynamicAllocation.initialExecutors\" : 0 , \"spark.blacklist.enabled\" : false , \"properties\" : \"\" , // New-line separated entry such as \"spark.yarn.appMasterEnv.envvar=value\\nspark.yarn.appMasterEnv.envvar1=value2\" \"spark.yarn.dist.pyFiles\" : \"\" , // Comma-separated string of paths to .py files \"spark.yarn.dist.files\" : \"\" , // Comma-separated string of paths to additional application files \"spark.yarn.dist.jars\" : \"\" , // Comma-separated string of paths to additional jars that should be added to the classpath \"spark.yarn.dist.archives\" : \"\" , // Comma-separated string of paths to archives that should be added and extracted \"appName\" : \"{JOB_NAME}\" , // Name of the job \"defaultArgs\" : \"\" , // A string containing the arguments for the app } Python # The image below shows an example of a Python job advanced configuration page. If set, these configuration properties will override the default project-scoped job configuration properties described in the section above. You can set the following properties for Python jobs: Container memory: The amount of memory in MB to be allocated to the container running the Python program Container cores: The number of cores to be allocated to the container running the Python program Additional files: List of files that will be locally accessible by the application Python job configuration properties If you are creating the job programmatically, the following JSON is an example job configuration for a PYTHON job. pythonJobConfiguration.json { \"type\" : \"pythonJobConfiguration\" , \"appPath\" : \"hdfs:///Projects/{PROJECT_NAME}/{DATASET_NAME}/{APP_FILE}\" , \"files\" : \"\" , // Comma-separated string of paths to additional application files \"resourceConfig\" : { \"type\" : \"dockerResourcesConfiguration\" , \"cores\" : 1 , \"memory\" : 1024 , // Memory in megabytes to allocate for the application \"gpus\" : 0 // GPUs to allocate for the application }, \"logRedirection\" : true , \"appName\" : \"{JOB_NAME}\" , // Name of the job \"defaultArgs\" : \"\" , // A string containing the arguments for the app } You do not have to upload the Python program via the Hopsworks UI to run it. It can be done programmatically from a Python program by using the upload function of the dataset module of the hops Python library To do that, first generate an API key for your project, and then use the project.connect() function of the same library to connect to a project of your Hopsworks cluster and then dataset.upload() . Docker # Docker jobs can currently only be managed from the legacy Hopsworks user interface, see documentation at Docker jobs for details. Apache Flink # Flink jobs can currently only be managed from the legacy Hopsworks user interface, see documentation at Flink jobs for details. Alerts # You can attach Hopsworks alerts on a per-job basis if you want to receive notifications for job-specific events. Firstly, a Hopsworks administrator needs to setup alert channels from the Cluster settings page. Currently supported channels are - email - slack - pagerduty Once you have configured the alert channels, you can proceed by specifying alert receivers from the project's settings page. Receivers are the destinations to which your alerts will be sent to. You can specify global alerts, which means these alerts will be triggered on job events regardless of which specific job generated the alert event. Alternatively, you can override the job alerts by adding alerts to a specific job from the job's configuration page as shown below. The job events alerts are triggered upon are: on job success on job fail on job killed The potential severities these alerts should report are: WARNING INFO CRITICAL Job alerts Edit & Delete # You can edit a job from the Job's overview page. If you would like to quickly create a job based on the configuration of another job, you can use the Make a copy button that will prompt you for a name for the new job and then immediately after create the new job. From the same page, you can also delete a job as shown in the image below. Please note that when deleting a job, Hopsworks will first attempt to gracefully terminate any pending executions of this job. All job logs will remain in the Logs dataset and it is up to the user to clean them up if needed. Job deletion Executions # You can execute a job multiple times concurrently with different input arguments. Each execution has an individual execution id that identifies it. Execution logs are stored under the Logs dataset and are categorized per job type, name, execution(Python)/application(Spark) id. Execute a job # You can execute a job either using the Quick run option from the job's preview page, as shown in the image below, or from the job's overview page. The difference between the two is that the latter will prompt you for input arguments whereas the former will use the default arguments, if any are provided. This page also shows a list of the most recent executions. For a detailed list of executions for the job, you need navigate to the executions page by clicking the view all executions link. Job preview Executions overview By default, a job can have a maximum of 10000 executions. This limit can be increased/lowered by a Hopsworks administrator. Monitoring and Logging # The executions page enables you to filter executions for the job based on the execution's date and state. You can view detailed information about each execution by following the various monitoring and logging links provided with each execution. In particular, you get access to the following dashboards/views for every execution: Spark UI: Applies to Spark/PySpark jobs only, a new tab will open showing the Spark application Web UI (see docs ). RM UI: Applies to Spark/PySpark/Flink jobs only, a new tab will open showing the Apache Hadoop YARN page of the execution Kibana: Applies to Spark/PySpark/Flink jobs only, a new tab will open showing the execution logs in real-time as they are collected from all the containers the execution is distributed at. Logs: Shows the aggregated stdout/stderr logs of the execution. These logs are stored under the Logs dataset. Hopsworks IDE Plugin # It is also possible to work on jobs while developing in your IntelliJ/PyCharm IDE by installing the Hopsworks Plugin from the marketplace. Usage Open the Hopsworks Job Preferences UI for specifying user preferences under Settings -> Tools -> Hopsworks Job Preferences . Input the Hopsworks project preferences and job details you wish to work on. Open a Project and within the Project Explorer, right click on the program ( .jar, .py, .ipynb) you wish to execute as a job on Hopsworks. Different job actions possible are available in the context menu ( Create, Run, Stop, etc.) Actions Create: Create or update job as specified in Hopsworks Job Preferences Run: Uploads the program first to the HDFS path as specified and runs job Stop: Stops a job Delete: Deletes a job Job Execution Status / Job Execution Logs: Get the job status or logs respectively. You have the option of retrieving a particular job execution by specifying the execution id in the 'Hopsworks Job Preferences' UI, otherwise default is the last execution for the job name specified. Working with jobs from Hopsworks IntelliJ/PyCharm plugin","title":"Jobs"},{"location":"compute/jobs/#jobs","text":"All members of a project in Hopsworks can launch the following types of applications through a project's Jobs service: Python ( Hopsworks Enterprise only ) Apache Spark Apache Flink Docker ( Hopsworks Enterprise only ) Launching a job of any type is very similar process, what mostly differs between job types is the various configuration parameters each job type comes with.","title":"Jobs"},{"location":"compute/jobs/#manage-jobs","text":"You can quickly create a new job by following these steps: From the project menu click Jobs and then the New job button Select a file to be used as the program the job will execute. The file can be selected either from within the project, that means you need to select a file stored in one of the project's datasets, or select a file from your local workstation which will be uploaded to the Resources dataset by default. You can also select Jupyter notebooks as files to be executed. When running a job, Hopsworks will automatically convert the PySpark/Python notebook to a .py file and run it. The notebook is converted every time the job runs, which means changes in the notebook will be picked up by the job without having to update it. Fill in a name for the job. The name can be made of alphanumeric characters, dash or underscore. A job's name is unique across the project, that is no two jobs can have the same name. If you would like to explore more advanced options such as importing a job or setting alerts, then you need to click the Advanced options button shown in the image below and from that page you can configure the options shown in the sections below. New job quickstart","title":"Manage jobs"},{"location":"compute/jobs/#new-job-from-scratch","text":"In addition to the options available with the new job quickstart, from this menu you can set default arguments to be given as input to the job when executed. You can overwrite these arguments when running a job by providing input arguments for that particular execution of the job. For example, in the image below the default arguments \"a b c\" will be passed as space-separated arguments to the job. Job default arguments","title":"New job from scratch"},{"location":"compute/jobs/#import-an-existing-job","text":"Instead of creating a new job from scratch, you can create one based on another jobs' configuration. The latter can be exported by using the Export button from the job's Overview page as shown below. This action will download a .json file containing the job's specification which you can then import from the advanced options page. Job export","title":"Import an existing job"},{"location":"compute/jobs/#compute-configuration","text":"You can set a default configuration for each job type (currently only supported for Spark and Python ) at a project scope, from the Project settings - Compute configuration menu option. That enables you to have the job configuration parameters already filled in when creating a job and optionally override them for this particular job instance. Compute configuration page","title":"Compute Configuration"},{"location":"compute/jobs/#spark","text":"The image below shows an example of a Spark job advanced configuration page. If set, these configuration properties will override the default project-scoped job configuration properties described in the section above. You can set the following properties for Spark/PySpark jobs: Main class: For Spark jobs, the main class of the application Driver memory Driver virtual cores Executor memory Executor virtual cores Dynamic - Static: Run the Spark job in static or dynamic allocation mode (see official docs for details). Additional archives: List of zip or .tgz files that will be locally accessible by the application Additional jars: List of .jar files to add to the CLASSPATH of the application Additional Python dependencies: List of .py, .zip or .egg files that will be locally accessible by the application Additional files: List of files that will be locally accessible by the application Properties: Optional line-separates properties to be set for the Spark application. For example, to set environment variables for the driver you can set the properties as shown below spark.yarn.appMasterEnv.envvar=value spark.yarn.appMasterEnv.envvar2=value2 Spark job configuration properties If you are creating the job programmatically, the following JSON is an example job configuration for a PYSPARK job: sparkJobConfiguration.json { \"type\" : \"sparkJobConfiguration\" , \"amQueue\" : \"default\" , // Should be set to \"default\", represents name of YARN queue \"amMemory\" : 2048 , // Memory in megabytes to configure for Spark driver \"amVCores\" : 1 , // Virtual cores to configure for Spark driver \"jobType\" : \"PYSPARK\" , // Should be SPARK if application is a .jar file, otherwise PYSPARK if .py or .ipynb is configured \"appPath\" : \"hdfs:///Projects/{PROJECT_NAME}/{DATASET_NAME}/{APP_FILE}\" , // Path to application file to execute, supported files are .jar, .py or .ipynb \"mainClass\" : \"org.apache.spark.deploy.PythonRunner\" , // Must be set to \"org.apache.spark.deploy.PythonRunner\" for PYSPARK jobs, otherwise Main class of configured jar to run. \"spark.executor.instances\" : 1 , \"spark.executor.cores\" : 1 , \"spark.executor.memory\" : 2048 , \"spark.executor.gpus\" : 0 , \"spark.tensorflow.num.ps\" : 1 , \"spark.dynamicAllocation.enabled\" : true , \"spark.dynamicAllocation.minExecutors\" : 0 , \"spark.dynamicAllocation.maxExecutors\" : 1 , \"spark.dynamicAllocation.initialExecutors\" : 0 , \"spark.blacklist.enabled\" : false , \"properties\" : \"\" , // New-line separated entry such as \"spark.yarn.appMasterEnv.envvar=value\\nspark.yarn.appMasterEnv.envvar1=value2\" \"spark.yarn.dist.pyFiles\" : \"\" , // Comma-separated string of paths to .py files \"spark.yarn.dist.files\" : \"\" , // Comma-separated string of paths to additional application files \"spark.yarn.dist.jars\" : \"\" , // Comma-separated string of paths to additional jars that should be added to the classpath \"spark.yarn.dist.archives\" : \"\" , // Comma-separated string of paths to archives that should be added and extracted \"appName\" : \"{JOB_NAME}\" , // Name of the job \"defaultArgs\" : \"\" , // A string containing the arguments for the app }","title":"Spark"},{"location":"compute/jobs/#python","text":"The image below shows an example of a Python job advanced configuration page. If set, these configuration properties will override the default project-scoped job configuration properties described in the section above. You can set the following properties for Python jobs: Container memory: The amount of memory in MB to be allocated to the container running the Python program Container cores: The number of cores to be allocated to the container running the Python program Additional files: List of files that will be locally accessible by the application Python job configuration properties If you are creating the job programmatically, the following JSON is an example job configuration for a PYTHON job. pythonJobConfiguration.json { \"type\" : \"pythonJobConfiguration\" , \"appPath\" : \"hdfs:///Projects/{PROJECT_NAME}/{DATASET_NAME}/{APP_FILE}\" , \"files\" : \"\" , // Comma-separated string of paths to additional application files \"resourceConfig\" : { \"type\" : \"dockerResourcesConfiguration\" , \"cores\" : 1 , \"memory\" : 1024 , // Memory in megabytes to allocate for the application \"gpus\" : 0 // GPUs to allocate for the application }, \"logRedirection\" : true , \"appName\" : \"{JOB_NAME}\" , // Name of the job \"defaultArgs\" : \"\" , // A string containing the arguments for the app } You do not have to upload the Python program via the Hopsworks UI to run it. It can be done programmatically from a Python program by using the upload function of the dataset module of the hops Python library To do that, first generate an API key for your project, and then use the project.connect() function of the same library to connect to a project of your Hopsworks cluster and then dataset.upload() .","title":"Python"},{"location":"compute/jobs/#docker","text":"Docker jobs can currently only be managed from the legacy Hopsworks user interface, see documentation at Docker jobs for details.","title":"Docker"},{"location":"compute/jobs/#apache-flink","text":"Flink jobs can currently only be managed from the legacy Hopsworks user interface, see documentation at Flink jobs for details.","title":"Apache Flink"},{"location":"compute/jobs/#alerts","text":"You can attach Hopsworks alerts on a per-job basis if you want to receive notifications for job-specific events. Firstly, a Hopsworks administrator needs to setup alert channels from the Cluster settings page. Currently supported channels are - email - slack - pagerduty Once you have configured the alert channels, you can proceed by specifying alert receivers from the project's settings page. Receivers are the destinations to which your alerts will be sent to. You can specify global alerts, which means these alerts will be triggered on job events regardless of which specific job generated the alert event. Alternatively, you can override the job alerts by adding alerts to a specific job from the job's configuration page as shown below. The job events alerts are triggered upon are: on job success on job fail on job killed The potential severities these alerts should report are: WARNING INFO CRITICAL Job alerts","title":"Alerts"},{"location":"compute/jobs/#edit-delete","text":"You can edit a job from the Job's overview page. If you would like to quickly create a job based on the configuration of another job, you can use the Make a copy button that will prompt you for a name for the new job and then immediately after create the new job. From the same page, you can also delete a job as shown in the image below. Please note that when deleting a job, Hopsworks will first attempt to gracefully terminate any pending executions of this job. All job logs will remain in the Logs dataset and it is up to the user to clean them up if needed. Job deletion","title":"Edit &amp; Delete"},{"location":"compute/jobs/#executions","text":"You can execute a job multiple times concurrently with different input arguments. Each execution has an individual execution id that identifies it. Execution logs are stored under the Logs dataset and are categorized per job type, name, execution(Python)/application(Spark) id.","title":"Executions"},{"location":"compute/jobs/#execute-a-job","text":"You can execute a job either using the Quick run option from the job's preview page, as shown in the image below, or from the job's overview page. The difference between the two is that the latter will prompt you for input arguments whereas the former will use the default arguments, if any are provided. This page also shows a list of the most recent executions. For a detailed list of executions for the job, you need navigate to the executions page by clicking the view all executions link. Job preview Executions overview By default, a job can have a maximum of 10000 executions. This limit can be increased/lowered by a Hopsworks administrator.","title":"Execute a job"},{"location":"compute/jobs/#monitoring-and-logging","text":"The executions page enables you to filter executions for the job based on the execution's date and state. You can view detailed information about each execution by following the various monitoring and logging links provided with each execution. In particular, you get access to the following dashboards/views for every execution: Spark UI: Applies to Spark/PySpark jobs only, a new tab will open showing the Spark application Web UI (see docs ). RM UI: Applies to Spark/PySpark/Flink jobs only, a new tab will open showing the Apache Hadoop YARN page of the execution Kibana: Applies to Spark/PySpark/Flink jobs only, a new tab will open showing the execution logs in real-time as they are collected from all the containers the execution is distributed at. Logs: Shows the aggregated stdout/stderr logs of the execution. These logs are stored under the Logs dataset.","title":"Monitoring and Logging"},{"location":"compute/jobs/#hopsworks-ide-plugin","text":"It is also possible to work on jobs while developing in your IntelliJ/PyCharm IDE by installing the Hopsworks Plugin from the marketplace. Usage Open the Hopsworks Job Preferences UI for specifying user preferences under Settings -> Tools -> Hopsworks Job Preferences . Input the Hopsworks project preferences and job details you wish to work on. Open a Project and within the Project Explorer, right click on the program ( .jar, .py, .ipynb) you wish to execute as a job on Hopsworks. Different job actions possible are available in the context menu ( Create, Run, Stop, etc.) Actions Create: Create or update job as specified in Hopsworks Job Preferences Run: Uploads the program first to the HDFS path as specified and runs job Stop: Stops a job Delete: Deletes a job Job Execution Status / Job Execution Logs: Get the job status or logs respectively. You have the option of retrieving a particular job execution by specifying the execution id in the 'Hopsworks Job Preferences' UI, otherwise default is the last execution for the job name specified. Working with jobs from Hopsworks IntelliJ/PyCharm plugin","title":"Hopsworks IDE Plugin"},{"location":"compute/jupyter/","text":"Jupyter notebooks # Developing with Jupyter notebooks is provided as a service in Hopsworks as part of the compute section of a project which you can access from the main project menu. The image below shows the Jupyter service page in Hopsworks. Jupyter dashboard in Hopsworks From this page, you can configure various options and settings to start Jupyter with as described in the sections below. Configuration and start # Hopsworks supports both JupyterLab and classic Jupyter as Jupyter development frameworks. Clicking Run Jupyter , will start JupyterLab by default. You can then open classic Jupyter from within JupyterLab by navigating to Help - Launch Classic Notebook . Hopsworks will attempt to open JupyterLab in a new browser tab. You can also click the Open Jupyter button to open Jupyter in a new tab if your browser is blocking this operation by default. Classic Jupyter Jupyter server is using the same configuration options as the Jobs service to run notebooks. Currently, the Spark/PySpark and Python configuration options are supported. For more information on how to set these options, see the Jobs section and for more information on how to work with Spark and Jupyter in Hopsworks see the Jupyter + Spark on Hopsworks below. If you would like to switch between configurations, you will have to shut down the running Jupyter server first. After starting a Spark/PySpark notebook, you can view monitoring and logging information as shown in the image below. For Spark/PySpark notebooks, this information is identical to the information provided when running Spark/PySpark jobs from the Jobs dashboard (see Jobs section for details). You can start multiple notebooks from the same Jupyter server, however keep in mind that for each notebook new compute resources will be allocated based on the configuration options Jupyter was started with. Jupyter dashboard page showing currently running applications Recent notebooks # When you run a notebook, its Jupyter configuration is stored and attached to the notebook. You can use this configuration later to start the Jupyter notebook directly from the Jupyter dashboard by clicking Open in Jupyter under the Recent notebooks section of the dashboard. For example, in the image above you can see a notebook called hello_world.ipynb which you can start directly. Hopsworks will use the Jupyter server configuration the notebook ran with last time. Logs # It can be useful to look at the Jupyter server logs in case of errors as they can provide more details compared to the error notification that is shown in the Jupyter dashboard. For example if Jupyter cannot start, simply click the Server Logs button next to the Start or Stop button in the Jupyter dashboard. This will open a new tab ( make sure your browser does not block the new tab! ) with the Jupyter logs as shown in the figure below. Jupyter logs Auto-shutdown # Jupyter server is configured to automatically shut down after 6 hours. This functionality enables you to avoid excessive use of compute resources. You can modify the shut down limit by adding 6-hour chunks to the shutdown time. The dashboard also shows you how much time is currently left before the Jupyter server shuts down. Debug Jupyter installation # Jupyter is installed in the Python environment of your project. This means that if a dependency of Jupyter is removed, or an incorrect version is installed it may not work properly. If the Python environment ends up in a state with conflicting libraries installed then an alert will be shown in the Python libraries page under the Project settings menu. Jupyter + Spark on Hopsworks # As a user, you will just interact with the Jupyter notebooks, but below you can find a detailed explanation of the technology behind the scenes. When using Jupyter on Hopsworks, a library called sparkmagic _ is used to interact with the Hopsworks cluster. When you create a Jupyter notebook on Hopsworks, you first select a kernel . A kernel is simply a program that executes the code that you have in the Jupyter cells, you can think of it as a REPL-backend to your jupyter notebook that acts as a frontend. Sparkmagic works with a remote REST server for Spark, called livy , running inside the Hopsworks cluster. Livy is an interface that Jupyter-on-Hopsworks uses to interact with the cluster. When you run Jupyter cells using the pyspark kernel, the kernel will automatically send commands to livy in the background for executing the commands on the cluster. Thus, the work that happens in the background when you run a Jupyter cell is as follows: The code in the cell will first go to the kernel. Next, the kernel sends the code as an HTTP REST request to Livy. When receiving the REST request, Livy executes the code on the Spark driver in the cluster. If the code is regular python/scala/R code, it will run inside a python/scala/R interpreter on the Spark driver. If the code includes a spark command, using the spark session, a spark job will be launched on the cluster from the Spark driver. When the python/scala/R or spark execution is finished, the results are sent back from Livy to the pyspark kernel/sparkmagic. Finally, the pyspark kernel displays the result in the Jupyter notebook. The three Jupyter kernels we support on Hopsworks are: Spark, a kernel for executing scala code and interacting with the cluster through spark-scala PySpark, a kernel for executing python code and interacting with the cluster through pyspark SparkR, a kernel for executing R code and interacting with the cluster through spark-R By default, all files and folders created by Spark are group writable (i.e umask=007). If you want to change this default umask you can add additional spark property spark.hadoop.fs.permissions.umask-mode=<umask> in the Properties textbox of the Jupyter server configuration, before starting the jupyter server. In the rest of this tutorial we will focus on the pyspark kernel. PySpark notebooks # After you have started the Jupyter notebook server, you can create a PySpark notebook from JupyterLab: Create a pyspark notebook When you execute the first cell in a PySpark notebook, the Spark session is automatically created, referring to the Hopsworks cluster. SparkSession creation with pyspark kernel The notebook will look just like any Python notebook, with the difference that the python interpreter is actually running on a Spark driver in the cluster. You can execute regular Python code: Executing python code on the spark driver in the cluster Since you are executing on the Spark driver, you can also run applications on spark executors in the cluster, the Spark session is available as the variable spark in the notebook: Starting a spark application from Jupyter When you execute a cell in Jupyter that starts a Spark job, you can go back to the Hopsworks Jupyter service dashboard where you can access the Spark web UI and other monitoring and logging tools. In addition to having access to a regular Python interpreter as well as the spark cluster, you also have access to magic commands provided by sparkmagic. You can view a list of all commands by executing a cell with %%help : Printing a list of all sparkmagic commands Plotting with PySpark Kernel # So far throughout this tutorial, the Jupyter notebook have behaved more or less identical to how it does if you start the notebook server locally on your machine using a Python kernel, without access to a Hopsworks cluster. However, there is one main difference from a user-standpoint when using PySpark notebooks instead of regular Python notebooks, this is related to plotting. Since the code in a PySpark notebook is executed remotely, in the Spark cluster, regular Python plotting will not work. What you can do however, is to use sparkmagic to download your remote spark dataframe as a local pandas dataframe and plot it using matplotlib , seaborn , or sparkmagic 's built-in visualization. To achieve this we use the magics: %%sql , %%spark , and %%local . The steps to do plotting using a PySpark notebook are illustrated below. Using this approach, you can have large scale cluster computation and plotting in the same notebook. Step 1 : Create a remote Spark Dataframe: Creating a spark dataframe Step 2 : Download the Spark Dataframe to a local Pandas Dataframe using %%sql or %%spark: Note : you should not try to download large spark dataframes for plotting. When you plot a dataframe, the entire dataframe must fit into memory, so add the flag \u2013maxrows x to limit the dataframe size when you download it to the local Jupyter server for plotting. Using %%sql : Downloading the spark dataframe to a pandas dataframe using %%sql Using %%spark : Downloading the spark dataframe to a pandas dataframe using %%spark Step 3 : Plot the pandas dataframe using Python plotting libraries: When you download a dataframe from Spark to Pandas with sparkmagic, it gives you a default visualization of the data using autovizwidget , as you saw in the screenshots above. However, sometimes you want custom plots, using matplotlib or seaborn. To do this, use the sparkmagic %%local to access the local pandas dataframe and then you can plot as usual. Import plotting libraries locally on the Jupyter server Plot a local pandas dataframe using seaborn and the magic %%local Plot a local pandas dataframe using matplotlib and the magic %%local Want to Learn More? # We provide a large number of example notebooks available at examples.hopsworks.ai with the notebook files themselves being hosted at the Hopsworks examples GitHub repository . Go to Hopsworks and try them out!","title":"Jupyter"},{"location":"compute/jupyter/#jupyter-notebooks","text":"Developing with Jupyter notebooks is provided as a service in Hopsworks as part of the compute section of a project which you can access from the main project menu. The image below shows the Jupyter service page in Hopsworks. Jupyter dashboard in Hopsworks From this page, you can configure various options and settings to start Jupyter with as described in the sections below.","title":"Jupyter notebooks"},{"location":"compute/jupyter/#configuration-and-start","text":"Hopsworks supports both JupyterLab and classic Jupyter as Jupyter development frameworks. Clicking Run Jupyter , will start JupyterLab by default. You can then open classic Jupyter from within JupyterLab by navigating to Help - Launch Classic Notebook . Hopsworks will attempt to open JupyterLab in a new browser tab. You can also click the Open Jupyter button to open Jupyter in a new tab if your browser is blocking this operation by default. Classic Jupyter Jupyter server is using the same configuration options as the Jobs service to run notebooks. Currently, the Spark/PySpark and Python configuration options are supported. For more information on how to set these options, see the Jobs section and for more information on how to work with Spark and Jupyter in Hopsworks see the Jupyter + Spark on Hopsworks below. If you would like to switch between configurations, you will have to shut down the running Jupyter server first. After starting a Spark/PySpark notebook, you can view monitoring and logging information as shown in the image below. For Spark/PySpark notebooks, this information is identical to the information provided when running Spark/PySpark jobs from the Jobs dashboard (see Jobs section for details). You can start multiple notebooks from the same Jupyter server, however keep in mind that for each notebook new compute resources will be allocated based on the configuration options Jupyter was started with. Jupyter dashboard page showing currently running applications","title":"Configuration and start"},{"location":"compute/jupyter/#recent-notebooks","text":"When you run a notebook, its Jupyter configuration is stored and attached to the notebook. You can use this configuration later to start the Jupyter notebook directly from the Jupyter dashboard by clicking Open in Jupyter under the Recent notebooks section of the dashboard. For example, in the image above you can see a notebook called hello_world.ipynb which you can start directly. Hopsworks will use the Jupyter server configuration the notebook ran with last time.","title":"Recent notebooks"},{"location":"compute/jupyter/#logs","text":"It can be useful to look at the Jupyter server logs in case of errors as they can provide more details compared to the error notification that is shown in the Jupyter dashboard. For example if Jupyter cannot start, simply click the Server Logs button next to the Start or Stop button in the Jupyter dashboard. This will open a new tab ( make sure your browser does not block the new tab! ) with the Jupyter logs as shown in the figure below. Jupyter logs","title":"Logs"},{"location":"compute/jupyter/#auto-shutdown","text":"Jupyter server is configured to automatically shut down after 6 hours. This functionality enables you to avoid excessive use of compute resources. You can modify the shut down limit by adding 6-hour chunks to the shutdown time. The dashboard also shows you how much time is currently left before the Jupyter server shuts down.","title":"Auto-shutdown"},{"location":"compute/jupyter/#debug-jupyter-installation","text":"Jupyter is installed in the Python environment of your project. This means that if a dependency of Jupyter is removed, or an incorrect version is installed it may not work properly. If the Python environment ends up in a state with conflicting libraries installed then an alert will be shown in the Python libraries page under the Project settings menu.","title":"Debug Jupyter installation"},{"location":"compute/jupyter/#jupyter-spark-on-hopsworks","text":"As a user, you will just interact with the Jupyter notebooks, but below you can find a detailed explanation of the technology behind the scenes. When using Jupyter on Hopsworks, a library called sparkmagic _ is used to interact with the Hopsworks cluster. When you create a Jupyter notebook on Hopsworks, you first select a kernel . A kernel is simply a program that executes the code that you have in the Jupyter cells, you can think of it as a REPL-backend to your jupyter notebook that acts as a frontend. Sparkmagic works with a remote REST server for Spark, called livy , running inside the Hopsworks cluster. Livy is an interface that Jupyter-on-Hopsworks uses to interact with the cluster. When you run Jupyter cells using the pyspark kernel, the kernel will automatically send commands to livy in the background for executing the commands on the cluster. Thus, the work that happens in the background when you run a Jupyter cell is as follows: The code in the cell will first go to the kernel. Next, the kernel sends the code as an HTTP REST request to Livy. When receiving the REST request, Livy executes the code on the Spark driver in the cluster. If the code is regular python/scala/R code, it will run inside a python/scala/R interpreter on the Spark driver. If the code includes a spark command, using the spark session, a spark job will be launched on the cluster from the Spark driver. When the python/scala/R or spark execution is finished, the results are sent back from Livy to the pyspark kernel/sparkmagic. Finally, the pyspark kernel displays the result in the Jupyter notebook. The three Jupyter kernels we support on Hopsworks are: Spark, a kernel for executing scala code and interacting with the cluster through spark-scala PySpark, a kernel for executing python code and interacting with the cluster through pyspark SparkR, a kernel for executing R code and interacting with the cluster through spark-R By default, all files and folders created by Spark are group writable (i.e umask=007). If you want to change this default umask you can add additional spark property spark.hadoop.fs.permissions.umask-mode=<umask> in the Properties textbox of the Jupyter server configuration, before starting the jupyter server. In the rest of this tutorial we will focus on the pyspark kernel.","title":"Jupyter + Spark on Hopsworks"},{"location":"compute/jupyter/#pyspark-notebooks","text":"After you have started the Jupyter notebook server, you can create a PySpark notebook from JupyterLab: Create a pyspark notebook When you execute the first cell in a PySpark notebook, the Spark session is automatically created, referring to the Hopsworks cluster. SparkSession creation with pyspark kernel The notebook will look just like any Python notebook, with the difference that the python interpreter is actually running on a Spark driver in the cluster. You can execute regular Python code: Executing python code on the spark driver in the cluster Since you are executing on the Spark driver, you can also run applications on spark executors in the cluster, the Spark session is available as the variable spark in the notebook: Starting a spark application from Jupyter When you execute a cell in Jupyter that starts a Spark job, you can go back to the Hopsworks Jupyter service dashboard where you can access the Spark web UI and other monitoring and logging tools. In addition to having access to a regular Python interpreter as well as the spark cluster, you also have access to magic commands provided by sparkmagic. You can view a list of all commands by executing a cell with %%help : Printing a list of all sparkmagic commands","title":"PySpark notebooks"},{"location":"compute/jupyter/#plotting-with-pyspark-kernel","text":"So far throughout this tutorial, the Jupyter notebook have behaved more or less identical to how it does if you start the notebook server locally on your machine using a Python kernel, without access to a Hopsworks cluster. However, there is one main difference from a user-standpoint when using PySpark notebooks instead of regular Python notebooks, this is related to plotting. Since the code in a PySpark notebook is executed remotely, in the Spark cluster, regular Python plotting will not work. What you can do however, is to use sparkmagic to download your remote spark dataframe as a local pandas dataframe and plot it using matplotlib , seaborn , or sparkmagic 's built-in visualization. To achieve this we use the magics: %%sql , %%spark , and %%local . The steps to do plotting using a PySpark notebook are illustrated below. Using this approach, you can have large scale cluster computation and plotting in the same notebook. Step 1 : Create a remote Spark Dataframe: Creating a spark dataframe Step 2 : Download the Spark Dataframe to a local Pandas Dataframe using %%sql or %%spark: Note : you should not try to download large spark dataframes for plotting. When you plot a dataframe, the entire dataframe must fit into memory, so add the flag \u2013maxrows x to limit the dataframe size when you download it to the local Jupyter server for plotting. Using %%sql : Downloading the spark dataframe to a pandas dataframe using %%sql Using %%spark : Downloading the spark dataframe to a pandas dataframe using %%spark Step 3 : Plot the pandas dataframe using Python plotting libraries: When you download a dataframe from Spark to Pandas with sparkmagic, it gives you a default visualization of the data using autovizwidget , as you saw in the screenshots above. However, sometimes you want custom plots, using matplotlib or seaborn. To do this, use the sparkmagic %%local to access the local pandas dataframe and then you can plot as usual. Import plotting libraries locally on the Jupyter server Plot a local pandas dataframe using seaborn and the magic %%local Plot a local pandas dataframe using matplotlib and the magic %%local","title":"Plotting with PySpark Kernel"},{"location":"compute/jupyter/#want-to-learn-more","text":"We provide a large number of example notebooks available at examples.hopsworks.ai with the notebook files themselves being hosted at the Hopsworks examples GitHub repository . Go to Hopsworks and try them out!","title":"Want to Learn More?"},{"location":"compute/python/","text":"Python environment # Hopsworks comes out of the box with a Python environment for data engineering, machine learning and more general data science development. There is one Python environment for each project. A library installed in the project's Python environment can be used in a Job or Jupyter notebook (Python or PySpark) run in the project. The environment ensures compatibility between the CUDA version and the installed TensorFlow and PyTorch versions for applications using NVIDIA GPUs. Navigating to the service # Managing the project's Python environment is provided as a service on Hopsworks and can be found in the Python libraries page under the Project settings menu. Open the Python libraries page When a project is created, the python 3.8 environment is automatically enabled. So no additional steps are required to start developing your machine learning application. Listing installed libraries # The list of installed libraries is displayed on the page. It is possible to filter based on the name and package manager to search for a particular library. Viewing installed libraries Installing libraries # Python packages can be installed from the following sources: PyPi, using pip package manager A conda channel, using conda package manager Packages saved in certain file formats, currently we support .whl or .egg A public or private git repository A requirements.txt file to install multiple libraries at the same time using pip An environment.yml file to install multiple libraries at the same time using conda and pip Install by name and version # Enter the name and, optionally, the desired version to install. Installing library by name and version Search and install # Enter the search term and select a library and version to install. Installing library using search Install from package file # Install a python package by uploading the corresponding package file and selecting it in the file browser. Currently supported formats include: .whl .egg requirements.txt environment.yml Installing library from file Install from .git repository # To install from a git repository simply provide the repository URL. The URL you should provide is the same as you would enter on the command line using pip install git+{repo_url} . In the case of a private git repository, also select whether it is a GitHub or GitLab repository and the preconfigured access token for the repository. Note : If you are installing from a git repository which is not GitHub or GitLab simply supply the access token in the URL. Keep in mind that in this case the access token may be visible in logs for other users in the same project to see. Installing library from git repo Uninstalling libraries # To uninstall a library, find the library in the list of installed libararies tab and click the trash icon to remove the library. Uninstall a library Debugging the environment # After each installation or uninstall of a library, the environment is analyzed to detect libraries that may not work properly. In order to do so we use the pip check tool, which is able to identify missing dependencies or if a dependency is installed with the incorrect version. The alert will automatically show if such an issue was found. Conflicts detected in environment Exporting an environment # An existing Anaconda environment can be exported as a yml file, clicking the Export env will download the environment.yml file in your browser. Export environment Recreating the environment # Sometimes it may be desirable to recreate the environment to start from the default project environment. In order to do that, first click Remove env . Remove environment After removing the environment, simply recreate it by clicking Create Environment . Create environment","title":"Python"},{"location":"compute/python/#python-environment","text":"Hopsworks comes out of the box with a Python environment for data engineering, machine learning and more general data science development. There is one Python environment for each project. A library installed in the project's Python environment can be used in a Job or Jupyter notebook (Python or PySpark) run in the project. The environment ensures compatibility between the CUDA version and the installed TensorFlow and PyTorch versions for applications using NVIDIA GPUs.","title":"Python environment"},{"location":"compute/python/#navigating-to-the-service","text":"Managing the project's Python environment is provided as a service on Hopsworks and can be found in the Python libraries page under the Project settings menu. Open the Python libraries page When a project is created, the python 3.8 environment is automatically enabled. So no additional steps are required to start developing your machine learning application.","title":"Navigating to the service"},{"location":"compute/python/#listing-installed-libraries","text":"The list of installed libraries is displayed on the page. It is possible to filter based on the name and package manager to search for a particular library. Viewing installed libraries","title":"Listing installed libraries"},{"location":"compute/python/#installing-libraries","text":"Python packages can be installed from the following sources: PyPi, using pip package manager A conda channel, using conda package manager Packages saved in certain file formats, currently we support .whl or .egg A public or private git repository A requirements.txt file to install multiple libraries at the same time using pip An environment.yml file to install multiple libraries at the same time using conda and pip","title":"Installing libraries"},{"location":"compute/python/#install-by-name-and-version","text":"Enter the name and, optionally, the desired version to install. Installing library by name and version","title":"Install by name and version"},{"location":"compute/python/#search-and-install","text":"Enter the search term and select a library and version to install. Installing library using search","title":"Search and install"},{"location":"compute/python/#install-from-package-file","text":"Install a python package by uploading the corresponding package file and selecting it in the file browser. Currently supported formats include: .whl .egg requirements.txt environment.yml Installing library from file","title":"Install from package file"},{"location":"compute/python/#install-from-git-repository","text":"To install from a git repository simply provide the repository URL. The URL you should provide is the same as you would enter on the command line using pip install git+{repo_url} . In the case of a private git repository, also select whether it is a GitHub or GitLab repository and the preconfigured access token for the repository. Note : If you are installing from a git repository which is not GitHub or GitLab simply supply the access token in the URL. Keep in mind that in this case the access token may be visible in logs for other users in the same project to see. Installing library from git repo","title":"Install from .git repository"},{"location":"compute/python/#uninstalling-libraries","text":"To uninstall a library, find the library in the list of installed libararies tab and click the trash icon to remove the library. Uninstall a library","title":"Uninstalling libraries"},{"location":"compute/python/#debugging-the-environment","text":"After each installation or uninstall of a library, the environment is analyzed to detect libraries that may not work properly. In order to do so we use the pip check tool, which is able to identify missing dependencies or if a dependency is installed with the incorrect version. The alert will automatically show if such an issue was found. Conflicts detected in environment","title":"Debugging the environment"},{"location":"compute/python/#exporting-an-environment","text":"An existing Anaconda environment can be exported as a yml file, clicking the Export env will download the environment.yml file in your browser. Export environment","title":"Exporting an environment"},{"location":"compute/python/#recreating-the-environment","text":"Sometimes it may be desirable to recreate the environment to start from the default project environment. In order to do that, first click Remove env . Remove environment After removing the environment, simply recreate it by clicking Create Environment . Create environment","title":"Recreating the environment"},{"location":"compute/auth/krb/","text":"Login using Kerberos # If Kerberos is configured you will see a Log in using alternative on the login page. Choose Kerberos and click on Go to Hopsworks to login. Log in using Kerberos If password login is disabled you only see the Log in using Kerberos/SSO alternative. Click on Go to Hopsworks to login. Kerberos only authentication To be able to authenticate with Kerberos you need to configure your browser to use Kerberos. Note that without a properly configured browser, the Kerberos token is not sent to the server and so SSO will not work. If Kerberos is not configured properly you will see Wrong credentials message when trying to log in. Missing Kerberos ticket When logging in with Kerberos for the first time Hopsworks will retrieve and save consented claims (firstname, lastname and email), about the logged in end-user. If you have multiple email addresses registered in Kerberos you can choose one to use with Hopsworks. If you do not want your information to be saved in Hopsworks you can click Cancel . This will redirect you back to the login page. Give consent After clicking on Register you will be redirected to the landing page: Landing page In the landing page, you will find two buttons. Use these buttons to either create a demo project or a new project .","title":"Kerberos Authentication"},{"location":"compute/auth/krb/#login-using-kerberos","text":"If Kerberos is configured you will see a Log in using alternative on the login page. Choose Kerberos and click on Go to Hopsworks to login. Log in using Kerberos If password login is disabled you only see the Log in using Kerberos/SSO alternative. Click on Go to Hopsworks to login. Kerberos only authentication To be able to authenticate with Kerberos you need to configure your browser to use Kerberos. Note that without a properly configured browser, the Kerberos token is not sent to the server and so SSO will not work. If Kerberos is not configured properly you will see Wrong credentials message when trying to log in. Missing Kerberos ticket When logging in with Kerberos for the first time Hopsworks will retrieve and save consented claims (firstname, lastname and email), about the logged in end-user. If you have multiple email addresses registered in Kerberos you can choose one to use with Hopsworks. If you do not want your information to be saved in Hopsworks you can click Cancel . This will redirect you back to the login page. Give consent After clicking on Register you will be redirected to the landing page: Landing page In the landing page, you will find two buttons. Use these buttons to either create a demo project or a new project .","title":"Login using Kerberos"},{"location":"compute/auth/ldap/","text":"Login using LDAP # If LDAP is configured you will see a Log in using alternative on the login page. Choose LDAP and type in your username and password then click on Login . Note that you need to use your LDAP credentials. Log in using LDAP When logging in with LDAP for the first time Hopsworks will retrieve and save consented claims (firstname, lastname and email), about the logged in end-user. If you have multiple email addresses registered in LDAP you can choose one to use with Hopsworks. If you do not want your information to be saved in Hopsworks you can click Cancel . This will redirect you back to the login page. Give consent After clicking on Register you will be redirected to the landing page: Landing page In the landing page, you will find two buttons. Use these buttons to either create a demo project or a new project .","title":"LDAP Authentication"},{"location":"compute/auth/ldap/#login-using-ldap","text":"If LDAP is configured you will see a Log in using alternative on the login page. Choose LDAP and type in your username and password then click on Login . Note that you need to use your LDAP credentials. Log in using LDAP When logging in with LDAP for the first time Hopsworks will retrieve and save consented claims (firstname, lastname and email), about the logged in end-user. If you have multiple email addresses registered in LDAP you can choose one to use with Hopsworks. If you do not want your information to be saved in Hopsworks you can click Cancel . This will redirect you back to the login page. Give consent After clicking on Register you will be redirected to the landing page: Landing page In the landing page, you will find two buttons. Use these buttons to either create a demo project or a new project .","title":"Login using LDAP"},{"location":"compute/auth/login/","text":"Login to Hopsworks # After your account is validated by an administrator you can use your email and password to login. Login with password If second factor authentication is enabled you will be presented with a second factor authentication window after you enter your password. Use your authenticator app (example. Google Authenticator ) on your phone to get a one-time password. One time password Upon successful login, you will arrive at the landing page: Landing page In the landing page, you will find two buttons. Use these buttons to either create a demo project or a new project .","title":"Login"},{"location":"compute/auth/login/#login-to-hopsworks","text":"After your account is validated by an administrator you can use your email and password to login. Login with password If second factor authentication is enabled you will be presented with a second factor authentication window after you enter your password. Use your authenticator app (example. Google Authenticator ) on your phone to get a one-time password. One time password Upon successful login, you will arrive at the landing page: Landing page In the landing page, you will find two buttons. Use these buttons to either create a demo project or a new project .","title":"Login to Hopsworks"},{"location":"compute/auth/oauth2/","text":"Login using a third-party identity provider. # If OAuth is configured a Login with button will appear in the login page. Use this button to log in to Hopsworks using your OAuth credentials. Login with OAuth2 When logging in with OAuth for the first time Hopsworks will retrieve and save consented claims (firstname, lastname and email), about the logged in end-user. Give consent After clicking on Register you will be redirected to the landing page: Landing page In the landing page, you will find two buttons. Use these buttons to either create a demo project or a new project .","title":"OAuth2 Authentication"},{"location":"compute/auth/oauth2/#login-using-a-third-party-identity-provider","text":"If OAuth is configured a Login with button will appear in the login page. Use this button to log in to Hopsworks using your OAuth credentials. Login with OAuth2 When logging in with OAuth for the first time Hopsworks will retrieve and save consented claims (firstname, lastname and email), about the logged in end-user. Give consent After clicking on Register you will be redirected to the landing page: Landing page In the landing page, you will find two buttons. Use these buttons to either create a demo project or a new project .","title":"Login using a third-party identity provider."},{"location":"compute/auth/recoverPassword/","text":"Password Recovery # If you forget your password click on Forgot password on the login page. Enter your email and click on the Send reset link button. Password reset A password reset link will be sent to the email address you entered if the email is found in the system. Click on the reset link to set your new password.","title":"Password Recovery"},{"location":"compute/auth/recoverPassword/#password-recovery","text":"If you forget your password click on Forgot password on the login page. Enter your email and click on the Send reset link button. Password reset A password reset link will be sent to the email address you entered if the email is found in the system. Click on the reset link to set your new password.","title":"Password Recovery"},{"location":"compute/auth/registration/","text":"Register a New Account on Hopsworks # The process for registering a new account is as follows: Click on the Register button on the login page. Register your email address and details. Validate your email address by clicking on the link in the validation email you received. Wait until an administrator has approved your account (you will receive a confirmation email). Register new account If second factor authentication is required you will be presented with a page like in the figure below. Scan the QR code or type the code in bold to register your account in your authenticator app (example. Google Authenticator ). Add second factor authentication After your account is created an administrator needs to validate your account before you can log in. Account created","title":"Registration"},{"location":"compute/auth/registration/#register-a-new-account-on-hopsworks","text":"The process for registering a new account is as follows: Click on the Register button on the login page. Register your email address and details. Validate your email address by clicking on the link in the validation email you received. Wait until an administrator has approved your account (you will receive a confirmation email). Register new account If second factor authentication is required you will be presented with a page like in the figure below. Scan the QR code or type the code in bold to register your account in your authenticator app (example. Google Authenticator ). Add second factor authentication After your account is created an administrator needs to validate your account before you can log in. Account created","title":"Register a New Account on Hopsworks"},{"location":"compute/auth/updateProfile/","text":"Update your Profile and Credentials # After you have logged in, in the upper right-hand corner of the screen, you will see your name. Click on your name, then click on the menu item Account settings . The User settings page will open with profile tab selected. In this tab you can change your first and last name. You cannot change your email address and will need to create a new account if you wish to change your email address. You can also log out by clicking on the Log out menu item. Update profile Update credential # To update your credential go to the Authentication tab as shown in the image below. Update credential Enable/Reset Two-factor Authentication # You can also change your two-factor setting in the Authentication tab. Second factor authentication is only available if it is enabled from the cluster administration page. Enable Two-factor Authentication After enabling or resetting two-factor you will be presented with a QR Code. You will then need to scan the QR code to add it on your phone's authenticator application (example. Google Authenticator ). If you miss this step, you will have to recover your smartphone credentials at a later stage. Register Two-factor Authentication Use the one time password generated by your authenticator app to confirm the registration.","title":"Update Profile"},{"location":"compute/auth/updateProfile/#update-your-profile-and-credentials","text":"After you have logged in, in the upper right-hand corner of the screen, you will see your name. Click on your name, then click on the menu item Account settings . The User settings page will open with profile tab selected. In this tab you can change your first and last name. You cannot change your email address and will need to create a new account if you wish to change your email address. You can also log out by clicking on the Log out menu item. Update profile","title":"Update your Profile and Credentials"},{"location":"compute/auth/updateProfile/#update-credential","text":"To update your credential go to the Authentication tab as shown in the image below. Update credential","title":"Update credential"},{"location":"compute/auth/updateProfile/#enablereset-two-factor-authentication","text":"You can also change your two-factor setting in the Authentication tab. Second factor authentication is only available if it is enabled from the cluster administration page. Enable Two-factor Authentication After enabling or resetting two-factor you will be presented with a QR Code. You will then need to scan the QR code to add it on your phone's authenticator application (example. Google Authenticator ). If you miss this step, you will have to recover your smartphone credentials at a later stage. Register Two-factor Authentication Use the one time password generated by your authenticator app to confirm the registration.","title":"Enable/Reset Two-factor Authentication"},{"location":"compute/project/createProject/","text":"Create a New Project # You can create a project by clicking on the Create new project button in the Projects list page. This will pop-up a dialog, in which you enter the project name and an optional description. You can also select an initial set of members for the project. The members you select will be given the role of Data Scientist in the project. Member roles can later be updated in the Project settings by the project owner or a member with the data owner role: for more information about the roles see the documentation here . A valid project name can only contain characters a-z, A-Z, 0-9 and special characters \u2018_\u2019 and \u2018.\u2019 but not \u2018__\u2019 (double underscore). There are also reserved words that are not allowed in project names. A complete list of reserved words can be found in section Project name reserved words . Create project As soon as you have created a new project, you can click on Open project in the project list, to see the project main page as illustrated in the figure Project overview. Project list Project overview On the left-hand side of the project overview page is the Project Menu. On the top we have the feature store section with feature groups, training datasets and storage connectors. In the middle we have the compute section containing Jupyter and Jobs . Finally, on the bottom of the menu we have the Configuration section with settings for the project. In Settings, you will find 4 sub categories that allow you to configure general configuration, python libraries, alerts and integrations to other services. From the general configuration you can add members, share feature store with another project and delete the project. Project Settings On the top navigation bar next to the Hopsworks logo we find the project name. By clicking on the project name you can go to other projects or back to the projects list page.","title":"Create a New Project"},{"location":"compute/project/createProject/#create-a-new-project","text":"You can create a project by clicking on the Create new project button in the Projects list page. This will pop-up a dialog, in which you enter the project name and an optional description. You can also select an initial set of members for the project. The members you select will be given the role of Data Scientist in the project. Member roles can later be updated in the Project settings by the project owner or a member with the data owner role: for more information about the roles see the documentation here . A valid project name can only contain characters a-z, A-Z, 0-9 and special characters \u2018_\u2019 and \u2018.\u2019 but not \u2018__\u2019 (double underscore). There are also reserved words that are not allowed in project names. A complete list of reserved words can be found in section Project name reserved words . Create project As soon as you have created a new project, you can click on Open project in the project list, to see the project main page as illustrated in the figure Project overview. Project list Project overview On the left-hand side of the project overview page is the Project Menu. On the top we have the feature store section with feature groups, training datasets and storage connectors. In the middle we have the compute section containing Jupyter and Jobs . Finally, on the bottom of the menu we have the Configuration section with settings for the project. In Settings, you will find 4 sub categories that allow you to configure general configuration, python libraries, alerts and integrations to other services. From the general configuration you can add members, share feature store with another project and delete the project. Project Settings On the top navigation bar next to the Hopsworks logo we find the project name. By clicking on the project name you can go to other projects or back to the projects list page.","title":"Create a New Project"},{"location":"compute/project/deleteProject/","text":"Delete a Project # You can delete a project in the Project Settings. In the Project Settings General tab, you will find a Danger Zone section where there is an option to delete the project. Project Settings Warning Deleting a project will delete all the datasets, features and jobs owned by the project.","title":"Delete a Project"},{"location":"compute/project/deleteProject/#delete-a-project","text":"You can delete a project in the Project Settings. In the Project Settings General tab, you will find a Danger Zone section where there is an option to delete the project. Project Settings Warning Deleting a project will delete all the datasets, features and jobs owned by the project.","title":"Delete a Project"},{"location":"compute/project/demoProject/","text":"Start with A Demo Project # When you log in to Hopsworks for the first time you will be presented with a landing page as shown in the figure below. From the landing page you can either create a new project or start with a demo project. The demo project is a good place to start if you are new to Hopsworks. Landing page You can create a demo project by clicking on the Run a demo project button on the landing page or the create project page. Demo project After creating a demo project you can go in to the project by clicking on Open project . The demo project will create a job that in turn will create sample feature groups and training datasets. If you go to Jobs on the side menu you will find the job named featurestore_tour_job as shown in the figure below. Demo job Wait until the job succeeds. Job succeeded Once the job has succeeded you can go to Feature Groups on the side menu to inspect the created feature groups. The figure below shows some sample feature groups created by the demo job. Created feature groups Similarly, you can go to the Training dataset on the side menu to inspect the created training dataset. Created training dataset If you want to experiment with the feature groups and training datasets created in this demo, go to Jupyter tab on the side menu. From the Jupyter page you can start JupyterLab and explore the available notebooks. See Feature Store Notebooks for more examples on Feature Engineering, Feature Ingestion, Feature Selection/Joining, Training Dataset Creating, Model Training, and Model Serving. To learn how to create your own project go to Create a New Project","title":"Start with a Demo Project"},{"location":"compute/project/demoProject/#start-with-a-demo-project","text":"When you log in to Hopsworks for the first time you will be presented with a landing page as shown in the figure below. From the landing page you can either create a new project or start with a demo project. The demo project is a good place to start if you are new to Hopsworks. Landing page You can create a demo project by clicking on the Run a demo project button on the landing page or the create project page. Demo project After creating a demo project you can go in to the project by clicking on Open project . The demo project will create a job that in turn will create sample feature groups and training datasets. If you go to Jobs on the side menu you will find the job named featurestore_tour_job as shown in the figure below. Demo job Wait until the job succeeds. Job succeeded Once the job has succeeded you can go to Feature Groups on the side menu to inspect the created feature groups. The figure below shows some sample feature groups created by the demo job. Created feature groups Similarly, you can go to the Training dataset on the side menu to inspect the created training dataset. Created training dataset If you want to experiment with the feature groups and training datasets created in this demo, go to Jupyter tab on the side menu. From the Jupyter page you can start JupyterLab and explore the available notebooks. See Feature Store Notebooks for more examples on Feature Engineering, Feature Ingestion, Feature Selection/Joining, Training Dataset Creating, Model Training, and Model Serving. To learn how to create your own project go to Create a New Project","title":"Start with A Demo Project"},{"location":"compute/project/iamRoleChaining/","text":"Assuming AWS IAM Roles # When deploying Hopsworks on EC2 instances you might need to assume different roles to access resources on AWS. These roles are configured in AWS and mapped to a project in Hopsworks, for a guide on how to configure this go to AWS IAM Role Chaining . After an administrator configured role mappings in Hopsworks you can see the roles you can assume in the Project Settings IAM Role Chaining tab. Role Chaining You can then use the Hops python library and Hops java/scala library to assume the roles listed in your project\u2019s settings page. When calling assume_role you can pass the role ARN string or use the get role method that takes the role id as an argument. If you assign a default role for your project you can call assume_role without arguments. You can assign (if you are a Data owner in that project) a default role to you project by clicking on the default checkbox of the role you want to make default. You can set one default per project role. If a default is set for a project role (Data scientist or Data owner) and all members (ALL) the default set for the project role will take precedence over the default set for all members. python # from hops.credentials_provider import get_role , assume_role credentials = assume_role ( role_arn = get_role ( 1 )) spark . read . csv ( \"s3a://resource/test.csv\" ) . show () scala # import io . hops . util . CredentialsProvider val creds = CredentialsProvider . assumeRole ( CredentialsProvider . getRole ( 1 )) spark . read . csv ( \"s3a://resource/test.csv\" ). show () The assume_role method sets spark hadoop configurations that will allow spark to read s3 buckets. The code examples above show how to read s3 buckets using Python and Scala. The method also sets environment variables AWS_ACCESS_KEY_ID , AWS_SECRET_ACCESS_KEY and AWS_SESSION_TOKEN so that programs running in the container can use the credentials for the newly assumed role.","title":"Assuming IAM Roles"},{"location":"compute/project/iamRoleChaining/#assuming-aws-iam-roles","text":"When deploying Hopsworks on EC2 instances you might need to assume different roles to access resources on AWS. These roles are configured in AWS and mapped to a project in Hopsworks, for a guide on how to configure this go to AWS IAM Role Chaining . After an administrator configured role mappings in Hopsworks you can see the roles you can assume in the Project Settings IAM Role Chaining tab. Role Chaining You can then use the Hops python library and Hops java/scala library to assume the roles listed in your project\u2019s settings page. When calling assume_role you can pass the role ARN string or use the get role method that takes the role id as an argument. If you assign a default role for your project you can call assume_role without arguments. You can assign (if you are a Data owner in that project) a default role to you project by clicking on the default checkbox of the role you want to make default. You can set one default per project role. If a default is set for a project role (Data scientist or Data owner) and all members (ALL) the default set for the project role will take precedence over the default set for all members.","title":"Assuming AWS IAM Roles"},{"location":"compute/project/iamRoleChaining/#python","text":"from hops.credentials_provider import get_role , assume_role credentials = assume_role ( role_arn = get_role ( 1 )) spark . read . csv ( \"s3a://resource/test.csv\" ) . show ()","title":"python"},{"location":"compute/project/iamRoleChaining/#scala","text":"import io . hops . util . CredentialsProvider val creds = CredentialsProvider . assumeRole ( CredentialsProvider . getRole ( 1 )) spark . read . csv ( \"s3a://resource/test.csv\" ). show () The assume_role method sets spark hadoop configurations that will allow spark to read s3 buckets. The code examples above show how to read s3 buckets using Python and Scala. The method also sets environment variables AWS_ACCESS_KEY_ID , AWS_SECRET_ACCESS_KEY and AWS_SESSION_TOKEN so that programs running in the container can use the credentials for the newly assumed role.","title":"scala"},{"location":"compute/project/multiTenancy/","text":"Project-based multi-tenancy # A project can be thought of as an abstraction combining three entities: Data Users Compute management (Jobs, Notebooks ...) A user in a project is called a member of the project and can carry one of two roles: Data Owner Data Scientist Data Owners can perform all kinds of operations in project, most importantly invite members, create and share Datasets, delete data in a project. Data Scientists are given mostly read-only access to Datasets and are not allowed to invite other members. This role is mostly useful for inviting members from other departments of an organization, so they can apply their AI pipelines and develop programs on the data that is owned by the Data Owners. A Data Owner can invite other members by clicking Add members button on the right-hand side of the Members card in the general settings page. Then search by email of the user to invite. The figure below shows a project project1 with two members, a Data Owner (Admin) and a Data Scientist (OnlineFS). The role of a member can be altered at any time from the same screen by any Data Owner by clicking on manage members link. Add a new member Warning OnlineFS is a system user, so deleting or altering the role of this user can create problems in the online feature store.","title":"Project-based Multi-tenancy"},{"location":"compute/project/multiTenancy/#project-based-multi-tenancy","text":"A project can be thought of as an abstraction combining three entities: Data Users Compute management (Jobs, Notebooks ...) A user in a project is called a member of the project and can carry one of two roles: Data Owner Data Scientist Data Owners can perform all kinds of operations in project, most importantly invite members, create and share Datasets, delete data in a project. Data Scientists are given mostly read-only access to Datasets and are not allowed to invite other members. This role is mostly useful for inviting members from other departments of an organization, so they can apply their AI pipelines and develop programs on the data that is owned by the Data Owners. A Data Owner can invite other members by clicking Add members button on the right-hand side of the Members card in the general settings page. Then search by email of the user to invite. The figure below shows a project project1 with two members, a Data Owner (Admin) and a Data Scientist (OnlineFS). The role of a member can be altered at any time from the same screen by any Data Owner by clicking on manage members link. Add a new member Warning OnlineFS is a system user, so deleting or altering the role of this user can create problems in the online feature store.","title":"Project-based multi-tenancy"},{"location":"compute/project/reservedNames/","text":"Project Name Reserved Words # A valid project name can only contain characters a-z, A-Z, 0-9 and special characters \u2018_\u2019 and \u2018.\u2019 but not \u2018__\u2019 (double underscore). There are also reserved words that are not allowed in project names. The following list provides all the reserved words: PROJECTS, HOPS-SYSTEM, HOPSWORKS, INFORMATION_SCHEMA, AIRFLOW, GRAFANA, MYSQL, NDBINFO, PERFORMANCE_SCHEMA, SQOOP, SYS, GLASSFISH_TIMERS, HOPS, METASTORE, BIGINT, BINARY, BOOLEAN, BOTH, BY, CASE, CAST, CHAR, COLUMN, CONF, CREATE, CROSS, CUBE, CURRENT, CURRENT_DATE, CURRENT_TIMESTAMP, CURSOR, DATABASE, DATE, DECIMAL, DELETE, DISTINCT, DOUBLE, DROP, ELSE, END, EXCHANGE, EXISTS, EXTENDED, EXTERNAL, FALSE, FETCH, FLOAT, FOLLOWING, FOR, FROM, FULL, FUNCTION, GRANT, GROUP, GROUPING, HAVING, IF, IMPORT, IN, INNER, INSERT, INT, INTERSECT, INTERVAL, INTO, IS, JOIN, LATERAL, LEFT, LESS, LIKE, LOCAL, MACRO, MAP, MORE, NONE, NOT, NULL, OF, ON, OR, ORDER, OUT, OUTER, OVER, PARTIALSCAN, PARTITION, PERCENT, PRECEDING, PRESERVE, PROCEDURE, RANGE, READS, REDUCE, REVOKE, RIGHT, ROLLUP, ROW, ROWS, SELECT, SET, SMALLINT, TABLE, TABLESAMPLE, THEN, TIMESTAMP, TO, TRANSFORM, TRIGGER, TRUE, TRUNCATE, UNBOUNDED, UNION, UNIQUEJOIN, UPDATE, USER, USING, UTC_TMESTAMP, VALUES, VARCHAR, WHEN, WHERE, WINDOW, WITH, COMMIT, ONLY, REGEXP, RLIKE, ROLLBACK, START, CACHE, CONSTRAINT, FOREIGN, PRIMARY, REFERENCES, DAYOFWEEK, EXTRACT, FLOOR, INTEGER, PRECISION, VIEWS, TIME, NUMERIC, SYNC, BASE, PYTHON37, PYTHON38, FILEBEAT, DESCRIBE. And any word containing _FEATURESTORE.","title":"Project Name Reserved Words"},{"location":"compute/project/reservedNames/#project-name-reserved-words","text":"A valid project name can only contain characters a-z, A-Z, 0-9 and special characters \u2018_\u2019 and \u2018.\u2019 but not \u2018__\u2019 (double underscore). There are also reserved words that are not allowed in project names. The following list provides all the reserved words: PROJECTS, HOPS-SYSTEM, HOPSWORKS, INFORMATION_SCHEMA, AIRFLOW, GRAFANA, MYSQL, NDBINFO, PERFORMANCE_SCHEMA, SQOOP, SYS, GLASSFISH_TIMERS, HOPS, METASTORE, BIGINT, BINARY, BOOLEAN, BOTH, BY, CASE, CAST, CHAR, COLUMN, CONF, CREATE, CROSS, CUBE, CURRENT, CURRENT_DATE, CURRENT_TIMESTAMP, CURSOR, DATABASE, DATE, DECIMAL, DELETE, DISTINCT, DOUBLE, DROP, ELSE, END, EXCHANGE, EXISTS, EXTENDED, EXTERNAL, FALSE, FETCH, FLOAT, FOLLOWING, FOR, FROM, FULL, FUNCTION, GRANT, GROUP, GROUPING, HAVING, IF, IMPORT, IN, INNER, INSERT, INT, INTERSECT, INTERVAL, INTO, IS, JOIN, LATERAL, LEFT, LESS, LIKE, LOCAL, MACRO, MAP, MORE, NONE, NOT, NULL, OF, ON, OR, ORDER, OUT, OUTER, OVER, PARTIALSCAN, PARTITION, PERCENT, PRECEDING, PRESERVE, PROCEDURE, RANGE, READS, REDUCE, REVOKE, RIGHT, ROLLUP, ROW, ROWS, SELECT, SET, SMALLINT, TABLE, TABLESAMPLE, THEN, TIMESTAMP, TO, TRANSFORM, TRIGGER, TRUE, TRUNCATE, UNBOUNDED, UNION, UNIQUEJOIN, UPDATE, USER, USING, UTC_TMESTAMP, VALUES, VARCHAR, WHEN, WHERE, WINDOW, WITH, COMMIT, ONLY, REGEXP, RLIKE, ROLLBACK, START, CACHE, CONSTRAINT, FOREIGN, PRIMARY, REFERENCES, DAYOFWEEK, EXTRACT, FLOOR, INTEGER, PRECISION, VIEWS, TIME, NUMERIC, SYNC, BASE, PYTHON37, PYTHON38, FILEBEAT, DESCRIBE. And any word containing _FEATURESTORE.","title":"Project Name Reserved Words"}]}