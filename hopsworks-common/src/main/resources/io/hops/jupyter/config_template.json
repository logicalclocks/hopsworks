{
  "kernel_python_credentials": {
    "username": "",
    "password": "",
    "url": "http://%%livy_ip%%:8998"
  },
  "kernel_scala_credentials": {
    "username": "",
    "password": "",
    "url": "http://%%livy_ip%%:8998"
  },
  "kernel_r_credentials": {
    "username": "",
    "password": "",
    "url": "http://%%livy_ip%%:8998"
  },
  "logging_config": {
    "version": 1,
    "formatters": {
      "magicsFormatter": {
        "format": "%(asctime)s\t%(levelname)s\t%(message)s",
        "datefmt": ""
      }
    },
    "handlers": {
      "magicsHandler": {
        "class": "hdijupyterutils.filehandler.MagicsFileHandler",
        "formatter": "magicsFormatter",
        "home_path": "%%jupyter_home%%/.sparkmagic"
      }
    },
    "loggers": {
      "magicsLogger": {
        "handlers": ["magicsHandler"],
        "level": "WARN",
        "propagate": 0
      }
    }
  },
  "wait_for_idle_timeout_seconds": 15,
  "status_sleep_seconds": 2,
  "statement_sleep_seconds": 2,
  "livy_session_startup_timeout_seconds": 240,
  "fatal_error_suggestion": "The code failed because of a fatal error:\n\t{}.\n\nSome things to try:\na) Make sure Spark has enough available resources for Jupyter to create a Spark context.\nb) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\nc) Restart the kernel.",
  "ignore_ssl_errors": false,
  "session_configs": {
    "driverCores": %%driver_cores%%,
    "driverMemory": "%%driver_memory%%",
    "numExecutors": %%num_executors%%,
    "executorCores": %%executor_cores%%,
    "executorMemory": "%%executor_memory%%",
    "proxyUser": "%%hdfs_user%%",
    "name": "%%spark_magic_name%%",
    "queue": "%%yarn_queue%%",
    "conf": {
      "spark.executorEnv.PATH": "%%spark_executorEnv_PATH%%",
      "spark.yarn.appMasterEnv.PYSPARK_PYTHON": "%%pyspark_bin%%",
      "spark.yarn.appMasterEnv.PYSPARK_DRIVER_PYTHON": "%%pyspark_bin%%",
      "spark.yarn.appMasterEnv.PYSPARK3_PYTHON": "%%pyspark_bin%%",
      "spark.yarn.appMasterEnv.LD_LIBRARY_PATH": "%%spark_yarn_appMaster_LD_LIBRARY_PATH%%",
      "spark.yarn.appMasterEnv.CUDA_VISIBLE_DEVICES": "",
      "spark.yarn.appMasterEnv.HADOOP_HOME": "%%hadoop_home%%",
      "spark.yarn.appMasterEnv.LIBHDFS_OPTS": "%%spark_yarn_appMasterEnv_LIBHDFS_OPTS%%",
      "spark.yarn.appMasterEnv.HADOOP_HDFS_HOME": "%%hadoop_home%%",
      "spark.yarn.appMasterEnv.HADOOP_USER_NAME": "%%hdfs_user%%",
      "spark.yarn.appMasterEnv.HDFS_BASE_DIR": "%%spark_yarn_appMasterEnv_HDFS_BASE_DIR%%",
      "spark.yarn.appMasterEnv.REST_ENDPOINT": "%%rest_endpoint%%",
      "spark.yarn.appMasterEnv.HADOOP_VERSION": "%%hadoop_version%%",
      "spark.yarn.appMasterEnv.LIVY_VERSION": "%%livy_version%%",
      "spark.yarn.appMasterEnv.SPARK_VERSION": "%%spark_version%%",
      "spark.yarn.appMasterEnv.KAFKA_VERSION": "%%kafka_version%%",
      "spark.yarn.appMasterEnv.TENSORFLOW_VERSION": "%%tensorflow_version%%",
      "spark.yarn.appMasterEnv.CUDA_VERSION": "%%cuda_version%%",
      "spark.yarn.appMasterEnv.HOPSWORKS_VERSION": "%%hopsworks_version%%",
      "spark.yarn.appMasterEnv.KAFKA_BROKERS": "%%kafka_brokers%%",
      "spark.yarn.appMasterEnv.ELASTIC_ENDPOINT": "%%elastic_endpoint%%",
      "spark.yarn.appMasterEnv.HOPSWORKS_USER": "%%hopsworks_user%%",
      "spark.yarn.stagingDir": "%%spark_yarn_stagingDir%%",
      "spark.yarn.dist.files": "%%spark_yarn_dist_files%%",
      "spark.yarn.dist.archives": "%%spark_yarn_dist_archives%%",
      "spark.yarn.dist.pyFiles": "%%spark_yarn_dist_pyFiles%%",
      "spark.driver.extraLibraryPath": "%%spark_driver_extraLibraryPath%%",
      "spark.driver.extraJavaOptions": "%%spark_driver_extraJavaOptions%%",
      "spark.driver.extraClassPath" : "%%spark_driver_extraClassPath%%",
      "spark.executor.extraClassPath" : "%%spark_executor_extraClassPath%%",
      "spark.executorEnv.HADOOP_USER_NAME": "%%hdfs_user%%",
      "spark.executorEnv.REST_ENDPOINT": "%%rest_endpoint%%",
      "spark.executorEnv.HADOOP_HOME": "%%hadoop_home%%",
      "spark.executorEnv.LIBHDFS_OPTS": "%%spark_executorEnv_LIBHDFS_OPTS%%",
      "spark.executorEnv.PYSPARK_PYTHON": "%%pyspark_bin%%",
      "spark.executorEnv.PYSPARK3_PYTHON": "%%pyspark_bin%%",
      "spark.executorEnv.LD_LIBRARY_PATH": "%%spark_executorEnv_LD_LIBRARY_PATH%%",
      "spark.executorEnv.HADOOP_HDFS_HOME": "%%hadoop_home%%",
      "spark.executorEnv.HADOOP_VERSION": "%%hadoop_version%%",
      "spark.executorEnv.LIVY_VERSION": "%%livy_version%%",
      "spark.executorEnv.SPARK_VERSION": "%%spark_version%%",
      "spark.executorEnv.KAFKA_VERSION": "%%kafka_version%%",
      "spark.executorEnv.TENSORFLOW_VERSION": "%%tensorflow_version%%",
      "spark.executorEnv.CUDA_VERSION": "%%cuda_version%%",
      "spark.executorEnv.HOPSWORKS_VERSION": "%%hopsworks_version%%",
      "spark.executorEnv.KAFKA_BROKERS": "%%kafka_brokers%%",
      "spark.executorEnv.ELASTIC_ENDPOINT": "%%elastic_endpoint%%",
      "spark.executorEnv.HOPSWORKS_USER": "%%hopsworks_user%%",
      "spark.executor.extraJavaOptions": "%%spark_executor_extraJavaOptions%%",
      "spark.executorEnv.HDFS_BASE_DIR": "%%spark_executorEnv_HDFS_BASE_DIR%%",
      "spark.pyspark.python": "%%pyspark_bin%%",
      "spark.shuffle.service.enabled": "true",
      "spark.submit.deployMode": "cluster",
      "spark.tensorflow.application": "%%spark_tensorflow_application%%",
      "spark.tensorflow.num.ps": "%%spark_tensorflow_num_ps%%",
      "spark.executor.gpus": "%%spark_executor_gpus%%",
      "spark.dynamicAllocation.enabled": "%%spark_dynamicAllocation_enabled%%",
      "spark.dynamicAllocation.initialExecutors": "%%spark_dynamicAllocation_initialExecutors%%",
      "spark.dynamicAllocation.minExecutors": "%%spark_dynamicAllocation_minExecutors%%",
      "spark.dynamicAllocation.maxExecutors": "%%spark_dynamicAllocation_maxExecutors%%",
      "spark.dynamicAllocation.executorIdleTimeout": "%%spark_dynamicAllocation_executorIdleTimeout%%",
      "spark.blacklist.enabled": "%%spark_blacklist_enabled%%",
      "spark.blacklist.task.maxTaskAttemptsPerExecutor": "%%spark_max_task_attempts_per_executor%%",
      "spark.blacklist.task.maxTaskAttemptsPerNode": "%%spark_max_task_attempts_per_node%%",
      "spark.blacklist.stage.maxFailedTasksPerExecutor": "%%spark_stage_max_failed_tasks_per_executor%%",
      "spark.blacklist.stage.maxFailedExecutorsPerNode": "%%spark_stage_max_failed_executors_per_node%%",
      "spark.blacklist.application.maxFailedTasksPerExecutor": "%%spark_application_max_failed_tasks_per_executor%%",
      "spark.blacklist.application.maxFailedExecutorsPerNode": "%%spark_application_max_failed_executors_per_node%%",
      "spark.blacklist.killBlacklistedExecutors": "%%spark_kill_blacklisted_executors%%",
      "spark.task.maxFailures": "%%spark_task_max_failures%%"%%spark_user_defined_properties%%
    }
  },
  "use_auto_viz": true,
  "max_results_sql": 2500,
  "pyspark_dataframe_encoding": "utf-8",
  "heartbeat_refresh_seconds": 30,
  "livy_server_heartbeat_timeout_seconds": 0,
  "heartbeat_retry_seconds": 10,
  "server_extension_default_kernel_name": "pysparkkernel",
  "env": {
    "HADOOP_HOME": "%%hadoop_home%%",
    "HADOOP_HDFS_HOME": "%%hadoop_home%%",
    "HADOOP_CONF_DIR": "%%hadoop_home%%/etc/hadoop",
    "HADOOP_USER_NAME": "%%hdfs_user%%",
    "HADOOP_VERSION": "%%hadoop_version%%"
  }
}

